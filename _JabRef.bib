% Encoding: UTF-8

@InProceedings{Iosup2018,
  author     = {Alexandru Iosup and Alexandru Uta and Laurens Versluis and Georgios Andreadis and Erwin Van Eyk and Tim Hegeman and Sacheendra Talluri and Vincent van Beek and Lucian Toader},
  booktitle  = {38th {IEEE} International Conference on Distributed Computing Systems, {ICDCS} 2018, Vienna, Austria, July 2-6, 2018},
  title      = {Massivizing Computer Systems: {A} Vision to Understand, Design, and Engineer Computer Ecosystems Through and Beyond Modern Distributed Systems},
  year       = {2018},
  pages      = {1224--1237},
  publisher  = {{IEEE} Computer Society},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/conf/icdcs/IosupUVAEHTBT18.bib},
  doi        = {10.1109/ICDCS.2018.00122},
  file       = {:Iosup2018 - Massivizing Computer Systems_ a Vision to Understand, Design, and Engineer Computer Ecosystems through and beyond Modern Distributed Systems.pdf:PDF},
  keywords   = {comp, distr},
  readstatus = {skimmed},
}

@Article{Davison2008,
  author    = {Andrew P Davison},
  journal   = {Frontiers in Neuroinformatics},
  title     = {{PyNN}: a common interface for neuronal network simulators},
  year      = {2008},
  volume    = {2},
  doi       = {10.3389/neuro.11.011.2008},
  file      = {:Davison2008 - PyNN_ a Common Interface for Neuronal Network Simulators.pdf:PDF},
  keywords  = {ann, snn, neuromorphic, alg, sim},
  publisher = {Frontiers Media {SA}},
}

@InProceedings{He2019,
  author     = {Xu He and Tianlin Liu and Fatemeh Hadaeghi and Herbert Jaeger},
  booktitle  = {2019 9th International {IEEE}/{EMBS} Conference on Neural Engineering ({NER})},
  title      = {Reservoir Transfer on Analog Neuromorphic Hardware},
  year       = {2019},
  month      = {mar},
  publisher  = {{IEEE}},
  abstract   = {Analog, unclocked, spiking neuromorphic mi- different objectives in these fields, LSM models are typically
crochips open new perspectives for implantable or wearable built around more or less detailed, spiking neuron models
biosensors and biocontrollers, due to their low energy con- with biologically plausible parametrizations, while ESNs
sumption and heat dissipation. However, the challenges from a
computational point of view are formidable. Here we outline mostly use highly abstracted rate models for its neurons.
our solutions to realize the reservoir computing paradigm on An RC architecture is composed of three major parts: the
such hardware and address the combined problems of low bit input layer feeds the input signal into a random, large, fixed
resolution, device mismatch, approximate neuron models, and recurrent neural network that constitutes the reservoir, from
timescale mismatch. The main contribution is a computational which the neurons in the output layer read out a desired
scheme, called Reservoir Transfer, which enables us to transfer
the dynamical properties of a well-performing neural network output signal. In traditional (and “deep”) RNN training meth-
which has been optimized on a digital computer, onto neuromor- ods, all synaptic weights in a neural learning architecture are
phic hardware that displays the abovementioned problematic optimized by descent on the output error gradient. In contrast,
properties. Here we present a case study of implementing an the input-to-reservoir and the recurrent reservoir-to-reservoir
ECG heartbeat abnormality detector to showcase the proposed weights in an RC system are left unchanged after a random
method.},
  doi        = {10.1109/ner.2019.8716891},
  file       = {:He2019 - Reservoir Transfer on Analog Neuromorphic Hardware.pdf:PDF},
  keywords   = {neuromorphic, analog, reservoir},
  readstatus = {read},
}

@Article{Indiveri2011,
  author    = {Giacomo Indiveri and Bernab{\'{e}} Linares-Barranco and Tara Julia Hamilton and Andr{\'{e}} van Schaik and Ralph Etienne-Cummings and Tobi Delbruck and Shih-Chii Liu and Piotr Dudek and Philipp Häfliger and Sylvie Renaud and Johannes Schemmel and Gert Cauwenberghs and John Arthur and Kai Hynna and Fopefolu Folowosele and Sylvain Saighi and Teresa Serrano-Gotarredona and Jayawan Wijekoon and Yingxue Wang and Kwabena Boahen},
  journal   = {Frontiers in Neuroscience},
  title     = {Neuromorphic Silicon Neuron Circuits},
  year      = {2011},
  volume    = {5},
  doi       = {10.3389/fnins.2011.00073},
  file      = {:Indiveri2011 - Neuromorphic Silicon Neuron Circuits.pdf:PDF},
  keywords  = {neuromorphic, hardware},
  priority  = {prio1},
  publisher = {Frontiers Media {SA}},
}

@Article{Thompson2009,
  author    = {B.C. Thompson and J.V. Tucker and J.I. Zucker},
  journal   = {Applied Mathematics and Computation},
  title     = {Unifying computers and dynamical systems using the theory of synchronous concurrent algorithms},
  year      = {2009},
  month     = {oct},
  number    = {4},
  pages     = {1386--1403},
  volume    = {215},
  doi       = {10.1016/j.amc.2009.04.058},
  file      = {:Thompson2009 - Unifying Computers and Dynamical Systems Using the Theory of Synchronous Concurrent Algorithms.pdf:PDF},
  keywords  = {dynsys, alg, comp},
  publisher = {Elsevier {BV}},
}

@InProceedings{Chicca2014,
  author    = {Elisabetta Chicca and Fabio Stefanini and Chiara Bartolozzi and Giacomo Indiveri},
  title     = {Neuromorphic Electronic Circuits for Building Autonomous Cognitive Systems},
  year      = {2014},
  month     = {sep},
  number    = {9},
  pages     = {1367--1388},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume    = {102},
  abstract  = {Several analog and digital brain-inspired electronic and massively parallel mechanisms and make extensive use of
systems have been recently proposed as dedicated solutions for adaptation, self-organization and learning.
fast simulations of spiking neural networks. While these archi- Several approaches have been recently proposed for build-
tectures are useful for exploring the computational properties
of large-scale models of the nervous system, the challenge of ing custom hardware, brain-like neural processing architec-
building low-power compact physical artifacts that can behave tures [1]–[9]. The majority of them are proposed as an
intelligently in the real-world and exhibit cognitive abilities alternative electronic substrate to traditional computing ar-
still remains open. In this paper we propose a set of neu- chitectures for neural simulations [2], [4], [5], [7]. These
romorphic engineering solutions to address this challenge. In systems can be very useful tools for neuroscience modeling,
particular, we review neuromorphic circuits for emulating neural
and synaptic dynamics in real-time and discuss the role of e.g., by accelerating the simulation of complex computational
biophysically realistic temporal dynamics in hardware neural neuroscience models by three or more orders of magnitude [4],
processing architectures; we review the challenges of realizing [7], [10]. However, our work focuses on an alternative ap-
spike-based plasticity mechanisms in real physical systems and proach aimed at the realization of compact, real-time, and
present examples of analog electronic circuits that implement energy efficient computational devices that directly emulate
them; we describe the computational properties of recurrent
neural networks and show how neuromorphic Winner-Take-All the style of computation of the brain, using the physics of
circuits can implement working-memory and decision-making Silicon to reproduce the bio-physics of the neural tissue. This
mechanisms. We validate the neuromorphic approach proposed approach, on one hand, leads to the implementation of compact
with experimental results obtained from our own circuits and and low-power behaving systems ranging from brain-machine
systems, and argue how the circuits and networks presented in interfaces to autonomous robotic agents. On the other hand,
this work represent a useful set of components for efficiently and
elegantly implementing neuromorphic cognition. it serves as a basic research instrument for exploring the},
  doi       = {10.1109/jproc.2014.2313954},
  file      = {:Chicca2014 - Neuromorphic Electronic Circuits for Building Autonomous Cognitive Systems.pdf:PDF},
  journal   = {Proceedings of the {IEEE}},
  keywords  = {neuromorphic, hardware, cogn},
  priority  = {prio1},
}

@Article{Markovic2020,
  author    = {Danijela Markovi{\'{c}} and Julie Grollier},
  journal   = {Applied Physics Letters},
  title     = {Quantum neuromorphic computing},
  year      = {2020},
  month     = oct,
  number    = {15},
  pages     = {150501},
  volume    = {117},
  doi       = {10.1063/5.0020014},
  file      = {:Markovic2020 - Quantum Neuromorphic Computing.pdf:PDF},
  keywords  = {quantum, neuromorphic},
  publisher = {{AIP} Publishing},
}

@Article{Davies2018,
  author    = {Mike Davies and Narayan Srinivasa and Tsung-Han Lin and Gautham Chinya and Yongqiang Cao and Sri Harsha Choday and Georgios Dimou and Prasad Joshi and Nabil Imam and Shweta Jain and Yuyun Liao and Chit-Kwan Lin and Andrew Lines and Ruokun Liu and Deepak Mathaikutty and Steven McCoy and Arnab Paul and Jonathan Tse and Guruguhanathan Venkataramanan and Yi-Hsin Weng and Andreas Wild and Yoonseok Yang and Hong Wang},
  journal   = {{IEEE} Micro},
  title     = {Loihi: A Neuromorphic Manycore Processor with On-Chip Learning},
  year      = {2018},
  month     = {jan},
  number    = {1},
  pages     = {82--99},
  volume    = {38},
  doi       = {10.1109/mm.2018.112130359},
  file      = {:Davies2018 - Loihi_ a Neuromorphic Manycore Processor with on Chip Learning.pdf:PDF},
  keywords  = {neuromorphic, hardware},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Richardson2006,
  author    = {Matthew Richardson and Pedro Domingos},
  journal   = {Machine Learning},
  title     = {Markov logic networks},
  year      = {2006},
  month     = {jan},
  number    = {1-2},
  pages     = {107--136},
  volume    = {62},
  doi       = {10.1007/s10994-006-5833-1},
  file      = {:Richardson2006 - Markov Logic Networks.pdf:PDF},
  keywords  = {logic, graph},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Griffiths2020,
  author    = {Thomas L. Griffiths},
  journal   = {Trends in Cognitive Sciences},
  title     = {Understanding Human Intelligence through Human Limitations},
  year      = {2020},
  month     = {nov},
  number    = {11},
  pages     = {873--883},
  volume    = {24},
  doi       = {10.1016/j.tics.2020.09.001},
  file      = {:Griffiths2020 - Understanding Human Intelligence through Human Limitations.pdf:PDF},
  keywords  = {cogn},
  publisher = {Elsevier {BV}},
}

@Article{Hole2019,
  author    = {Kjell J. Hole and Subutai Ahmad},
  journal   = {Computer},
  title     = {Biologically Driven Artificial Intelligence},
  year      = {2019},
  month     = {aug},
  number    = {8},
  pages     = {72--75},
  volume    = {52},
  doi       = {10.1109/mc.2019.2917455},
  file      = {:Hole2019 - Biologically Driven Artificial Intelligence.pdf:PDF},
  keywords  = {bio, ai, ann, views},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Pfeiffer2018,
  author    = {Michael Pfeiffer and Thomas Pfeil},
  journal   = {Frontiers in Neuroscience},
  title     = {Deep Learning With Spiking Neurons: Opportunities and Challenges},
  year      = {2018},
  month     = {oct},
  volume    = {12},
  doi       = {10.3389/fnins.2018.00774},
  file      = {:Pfeiffer2018 - Deep Learning with Spiking Neurons_ Opportunities and Challenges.pdf:PDF},
  keywords  = {views, snn, survey, neuromorphic},
  publisher = {Frontiers Media {SA}},
}

@Article{Feynman1982,
  author     = {Richard P. Feynman},
  journal    = {International Journal of Theoretical Physics},
  title      = {Simulating physics with computers},
  year       = {1982},
  month      = {jun},
  number     = {6-7},
  pages      = {467--488},
  volume     = {21},
  doi        = {10.1007/bf02650179},
  file       = {:Feynman1982 - Simulating Physics with Computers.pdf:PDF},
  keywords   = {physics, quantum, comp, found},
  publisher  = {Springer Science and Business Media {LLC}},
  readstatus = {read},
}

@Article{Gershenfeld2011,
  author     = {Neil Gershenfeld},
  journal    = {Computing},
  title      = {Aligning the representation and reality of computation with asynchronous logic automata},
  year       = {2011},
  month      = {nov},
  number     = {2-4},
  pages      = {91--102},
  volume     = {93},
  doi        = {10.1007/s00607-011-0160-1},
  file       = {:Gershenfeld2011 - Aligning the Representation and Reality of Computation with Asynchronous Logic Automata.pdf:PDF},
  keywords   = {comp, logic, physics},
  publisher  = {Springer Science and Business Media {LLC}},
  readstatus = {read},
}

@Article{Popescu2006a,
  author        = {Popescu, George A. and Mahale, Tushar and Gershenfeld, Neil},
  journal       = {NIP & Digital Fabrication Conference},
  title         = {Digital materials for digital printing},
  year          = {2006},
  issn          = {2169-4451},
  number        = {3},
  pages         = {58-61},
  volume        = {2006},
  abstract      = {Conventional three-dimensional printing processes are material-dependent, and are irreversible. We present an alternative approach based on three-dimensional assembly of mass-produced two-dimensional components of digital material. This significantly enlarges the available material
set, allows reversible disassembly, and imposes constraints that reduce the accumulation of local positioning errors in constructing a global shape. Experimental work on material properties and dimensional scaling of the digital material will be presented, with application in assembling functional
structures. We propose that assembling digital material will be the future of 3-dimensional free-form fabrication of functional materials.},
  file          = {:Popescu2006a - Digital Materials for Digital Printing.pdf:PDF},
  keywords      = {physics, materials},
  parent_itemid = {infobike://ist/nipdf},
  publishercode = {ist},
  readstatus    = {read},
  url           = {https://www.ingentaconnect.com/content/ist/nipdf/2006/00002006/00000003/art00019},
}

@Article{Popescu2006,
  author        = {Popescu, George A. and Kunzler, Patrik and Gershenfeld, Neil},
  journal       = {NIP & Digital Fabrication Conference},
  title         = {Digital Printing of Digital Materials},
  year          = {2006},
  issn          = {2169-4451},
  number        = {3},
  pages         = {55-57},
  volume        = {2006},
  abstract      = {We present a printer that builds functional three-dimensional structures by reversible assembly of a discrete set of components, digital materials. This approach uses the components rather than a control system to impose the spatial and functional constraints. Printing
can be performed as a parallel rather than a linear process. The printing process is reversible for re-use of the pieces or for error correction at any point in the object's life. Error detection, error-reduction and error-tolerance during assembly allows for reliable, high throughput
printing. We are presenting development approaches to such a printing device.},
  file          = {:Popescu2006 - Digital Printing of Digital Materials.pdf:PDF},
  itemtype      = {ARTICLE},
  keywords      = {physics, materials},
  parent_itemid = {infobike://ist/nipdf},
  publishercode = {ist},
  readstatus    = {read},
  url           = {https://www.ingentaconnect.com/content/ist/nipdf/2006/00002006/00000003/art00018},
}

@Article{Zenke2021,
  author    = {Friedemann Zenke and Sander M. Boht{\'{e}} and Claudia Clopath and Iulia M. Com{\c{s}}a and Julian Göltz and Wolfgang Maass and Timoth{\'{e}}e Masquelier and Richard Naud and Emre O. Neftci and Mihai A. Petrovici and Franz Scherr and Dan F.M. Goodman},
  journal   = {Neuron},
  title     = {Visualizing a joint future of neuroscience and neuromorphic engineering},
  year      = {2021},
  month     = {feb},
  number    = {4},
  pages     = {571--575},
  volume    = {109},
  doi       = {10.1016/j.neuron.2021.01.009},
  file      = {:Zenke2021 - Visualizing a Joint Future of Neuroscience and Neuromorphic Engineering.pdf:PDF},
  keywords  = {neuroscience, neuromorphic, views, survey},
  priority  = {prio1},
  publisher = {Elsevier {BV}},
}

@Article{Marinis2019,
  author    = {Lorenzo De Marinis and Marco Cococcioni and Piero Castoldi and Nicola Andriolli},
  journal   = {{IEEE} Access},
  title     = {Photonic Neural Networks: A Survey},
  year      = {2019},
  pages     = {175827--175841},
  volume    = {7},
  doi       = {10.1109/access.2019.2957245},
  file      = {:Marinis2019 - Photonic Neural Networks_ a Survey.pdf:PDF},
  keywords  = {survey, photonics, ann, snn},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Abreu2019,
  author        = {Steven Abreu},
  title         = {Automated Architecture Design for Deep Neural Networks},
  year          = {2019},
  month         = aug,
  abstract      = {Machine learning has made tremendous progress in recent years and received large amounts of public attention. Though we are still far from designing a full artificially intelligent agent, machine learning has brought us many applications in which computers solve human learning tasks remarkably well. Much of this progress comes from a recent trend within machine learning, called deep learning. Deep learning models are responsible for many state-of-the-art applications of machine learning. Despite their success, deep learning models are hard to train, very difficult to understand, and often times so complex that training is only possible on very large GPU clusters. Lots of work has been done on enabling neural networks to learn efficiently. However, the design and architecture of such neural networks is often done manually through trial and error and expert knowledge. This thesis inspects different approaches, existing and novel, to automate the design of deep feedforward neural networks in an attempt to create less complex models with good performance that take away the burden of deciding on an architecture and make it more efficient to design and train such deep networks.},
  archiveprefix = {arXiv},
  eprint        = {1908.10714},
  file          = {:Abreu2019 - Automated Architecture Design for Deep Neural Networks.pdf:PDF},
  keywords      = {mine, automl, ann},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Anderson1972,
  author     = {P. W. Anderson},
  journal    = {Science},
  title      = {More Is Different},
  year       = {1972},
  issn       = {00368075, 10959203},
  number     = {4047},
  pages      = {393--396},
  volume     = {177},
  file       = {:Anderson1972 - More Is Different.pdf:PDF},
  keywords   = {views, found, complexity},
  publisher  = {American Association for the Advancement of Science},
  readstatus = {read},
  url        = {http://www.jstor.org/stable/1734697},
}

@Article{Appeltant2011,
  author     = {L. Appeltant and M.C. Soriano and G. Van der Sande and J. Danckaert and S. Massar and J. Dambre and B. Schrauwen and C.R. Mirasso and I. Fischer},
  journal    = {Nature Communications},
  title      = {Information processing using a single dynamical node as complex system},
  year       = {2011},
  month      = {sep},
  number     = {1},
  volume     = {2},
  doi        = {10.1038/ncomms1476},
  file       = {:Appeltant2011 - Information Processing Using a Single Dynamical Node As Complex System.pdf:PDF;:Appeltant2011S - Information Processing Using a Single Dynamical Node As Complex System.pdf:PDF},
  keywords   = {reservoir, dynsys},
  publisher  = {Springer Science and Business Media {LLC}},
  readstatus = {read},
}

@Article{Cheung2019,
  author     = {Vincent K.M. Cheung and Peter M.C. Harrison and Lars Meyer and Marcus T. Pearce and John-Dylan Haynes and Stefan Koelsch},
  journal    = {Current Biology},
  title      = {Uncertainty and Surprise Jointly Predict Musical Pleasure and Amygdala, Hippocampus, and Auditory Cortex Activity},
  year       = {2019},
  month      = {dec},
  number     = {23},
  pages      = {4084--4092.e4},
  volume     = {29},
  doi        = {10.1016/j.cub.2019.09.067},
  file       = {:Cheung2019 - Uncertainty and Surprise Jointly Predict Musical Pleasure and Amygdala, Hippocampus, and Auditory Cortex Activity.pdf:PDF},
  keywords   = {neuroscience, music},
  publisher  = {Elsevier {BV}},
  readstatus = {read},
}

@Article{Castro2007,
  author     = {Leandro Nunes de Castro},
  journal    = {Physics of Life Reviews},
  title      = {Fundamentals of natural computing: an overview},
  year       = {2007},
  month      = {mar},
  number     = {1},
  pages      = {1--36},
  volume     = {4},
  doi        = {10.1016/j.plrev.2006.10.002},
  file       = {:Castro2007 - Fundamentals of Natural Computing_ an Overview.pdf:PDF},
  keywords   = {unconv, comp, bio, analog},
  publisher  = {Elsevier {BV}},
  readstatus = {read},
}

@InProceedings{Dean2020,
  author     = {Jeffrey Dean},
  booktitle  = {2020 {IEEE} International Solid- State Circuits Conference - ({ISSCC})},
  title      = {The Deep Learning Revolution and Its Implications for Computer Architecture and Chip Design},
  year       = {2020},
  month      = {feb},
  publisher  = {{IEEE}},
  doi        = {10.1109/isscc19947.2020.9063049},
  file       = {:Dean2020 - 1.1 the Deep Learning Revolution and Its Implications for Computer Architecture and Chip Design.pdf:PDF},
  keywords   = {hardware, energy},
  readstatus = {read},
}

@Article{Doty2012,
  author     = {David Doty},
  journal    = {Communications of the {ACM}},
  title      = {Theory of algorithmic self-assembly},
  year       = {2012},
  month      = {dec},
  number     = {12},
  pages      = {78--88},
  volume     = {55},
  doi        = {10.1145/2380656.2380675},
  file       = {:Doty2012 - Theory of Algorithmic Self Assembly.pdf:PDF},
  keywords   = {sassembly, alg},
  publisher  = {Association for Computing Machinery ({ACM})},
  readstatus = {read},
}

@Article{Goldmann2020,
  author     = {Mirko Goldmann and Felix Köster and Kathy Lüdge and Serhiy Yanchuk},
  journal    = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  title      = {Deep time-delay reservoir computing: Dynamics and memory capacity},
  year       = {2020},
  month      = {sep},
  number     = {9},
  pages      = {093124},
  volume     = {30},
  doi        = {10.1063/5.0017974},
  file       = {:Goldmann2020 - Deep Time Delay Reservoir Computing_ Dynamics and Memory Capacity.pdf:PDF},
  keywords   = {reservoir, dynsys},
  publisher  = {{AIP} Publishing},
  readstatus = {read},
}

@Article{Kanerva2009,
  author     = {Pentti Kanerva},
  journal    = {Cognitive Computation},
  title      = {Hyperdimensional Computing: An Introduction to Computing in Distributed Representation with High-Dimensional Random Vectors},
  year       = {2009},
  month      = {jan},
  number     = {2},
  pages      = {139--159},
  volume     = {1},
  doi        = {10.1007/s12559-009-9009-8},
  file       = {:Kanerva2009 - Hyperdimensional Computing_ an Introduction to Computing in Distributed Representation with High Dimensional Random Vectors.pdf:PDF},
  keywords   = {comp, hyperdim, cogn},
  publisher  = {Springer Science and Business Media {LLC}},
  readstatus = {read},
}

@Article{Maass2015,
  author     = {Wolfgang Maass},
  journal    = {Proceedings of the {IEEE}},
  title      = {To Spike or Not to Spike: That Is the Question},
  year       = {2015},
  month      = {dec},
  number     = {12},
  pages      = {2219--2224},
  volume     = {103},
  doi        = {10.1109/jproc.2015.2496679},
  file       = {:Maass2015 - To Spike or Not to Spike_ That Is the Question.pdf:PDF},
  keywords   = {snn, ann, neuromorphic, views},
  publisher  = {Institute of Electrical and Electronics Engineers ({IEEE})},
  readstatus = {read},
}

@Article{Mitchell2006,
  author     = {Melanie Mitchell},
  journal    = {Artificial Intelligence},
  title      = {Complex systems: Network thinking},
  year       = {2006},
  month      = {dec},
  number     = {18},
  pages      = {1194--1212},
  volume     = {170},
  doi        = {10.1016/j.artint.2006.10.002},
  file       = {:Mitchell2006 - Complex Systems_ Network Thinking.pdf:PDF},
  keywords   = {complexity, net, views},
  publisher  = {Elsevier {BV}},
  readstatus = {read},
}

@Article{Shannon1941,
  author     = {Claude E. Shannon},
  journal    = {Journal of Mathematics and Physics},
  title      = {Mathematical Theory of the Differential Analyzer},
  year       = {1941},
  month      = {apr},
  number     = {1-4},
  pages      = {337--354},
  volume     = {20},
  doi        = {10.1002/sapm1941201337},
  file       = {:Shannon1941 - Mathematical Theory of the Differential Analyzer.pdf:PDF},
  keywords   = {found, math, comp, analog, theory},
  publisher  = {Wiley},
  readstatus = {read},
}

@Article{Siegelmann1994,
  author     = {Hava T Siegelmann and Eduardo D Sontag},
  journal    = {Theoretical Computer Science},
  title      = {Analog computation via neural networks},
  year       = {1994},
  issn       = {0304-3975},
  month      = {sep},
  number     = {2},
  pages      = {331--360},
  volume     = {131},
  doi        = {10.1016/0304-3975(94)90178-3},
  file       = {:Siegelmann1994 - Analog Computation Via Neural Networks.pdf:PDF},
  keywords   = {analog, ann, comp},
  language   = {English},
  publisher  = {Elsevier {BV}},
  readstatus = {read},
  zbl        = {0822.68029},
}

@Article{Siegelmann1991,
  author     = {Hava T. Siegelmann and Eduardo D. Sontag},
  journal    = {Applied Mathematics Letters},
  title      = {Turing computability with neural nets},
  year       = {1991},
  number     = {6},
  pages      = {77--80},
  volume     = {4},
  doi        = {10.1016/0893-9659(91)90080-f},
  file       = {:Siegelmann1991 - Turing Computability with Neural Nets.pdf:PDF},
  keywords   = {analog, ann, comp},
  publisher  = {Elsevier {BV}},
  readstatus = {read},
}

@Article{Siegelmann1995,
  author     = {H. T. Siegelmann},
  journal    = {Science},
  title      = {Computation Beyond the Turing Limit},
  year       = {1995},
  month      = {apr},
  number     = {5210},
  pages      = {545--548},
  volume     = {268},
  doi        = {10.1126/science.268.5210.545},
  file       = {:Siegelmann1995 - Computation beyond the Turing Limit.pdf:PDF},
  keywords   = {analog, ann, comp},
  publisher  = {American Association for the Advancement of Science ({AAAS})},
  readstatus = {read},
}

@Article{Turing1937,
  author     = {A. M. Turing},
  journal    = {Proceedings of the London Mathematical Society},
  title      = {On Computable Numbers, with an Application to the Entscheidungsproblem},
  year       = {1937},
  number     = {1},
  pages      = {230--265},
  volume     = {s2-42},
  doi        = {10.1112/plms/s2-42.1.230},
  file       = {:Turing1937 - On Computable Numbers, with an Application to the Entscheidungsproblem.pdf:PDF},
  keywords   = {found, comp, logic},
  publisher  = {Wiley},
  readstatus = {read},
}

@InCollection{Leeuwen2001,
  author     = {Jan van Leeuwen and Ji{\v{r}}{\'{\i}} Wiedermann},
  booktitle  = {{SOFSEM} 2001: Theory and Practice of Informatics},
  publisher  = {Springer Berlin Heidelberg},
  title      = {Beyond the Turing Limit: Evolving Interactive Systems},
  year       = {2001},
  pages      = {90--109},
  doi        = {10.1007/3-540-45627-9_8},
  file       = {:Leeuwen2001 - Beyond the Turing Limit_ Evolving Interactive Systems.pdf:PDF},
  keywords   = {comp, interact, evo},
  readstatus = {read},
}

@Article{Wegner1997,
  author     = {Peter Wegner},
  journal    = {Communications of the {ACM}},
  title      = {Why interaction is more powerful than algorithms},
  year       = {1997},
  month      = {may},
  number     = {5},
  pages      = {80--91},
  volume     = {40},
  doi        = {10.1145/253769.253801},
  file       = {:Wegner1997 - Why Interaction Is More Powerful Than Algorithms.pdf:PDF},
  keywords   = {interact, alg, comp},
  publisher  = {Association for Computing Machinery ({ACM})},
  readstatus = {read},
}

@Article{Wegner2003,
  author     = {Peter Wegner and Dina Goldin},
  journal    = {Communications of the {ACM}},
  title      = {Computation beyond turing machines},
  year       = {2003},
  month      = {apr},
  number     = {4},
  pages      = {100--102},
  volume     = {46},
  doi        = {10.1145/641205.641235},
  file       = {:Wegner2003 - Computation beyond Turing Machines.pdf:PDF},
  keywords   = {views, comp, interact},
  publisher  = {Association for Computing Machinery ({ACM})},
  readstatus = {read},
}

@Article{Zhou2020,
  author        = {Chuteng Zhou and Prad Kadambi and Matthew Mattina and Paul N. Whatmough},
  title         = {Noisy Machines: Understanding Noisy Neural Networks and Enhancing Robustness to Analog Hardware Errors Using Distillation},
  year          = {2020},
  month         = jan,
  abstract      = {The success of deep learning has brought forth a wave of interest in computer hardware design to better meet the high demands of neural network inference. In particular, analog computing hardware has been heavily motivated specifically for accelerating neural networks, based on either electronic, optical or photonic devices, which may well achieve lower power consumption than conventional digital electronics. However, these proposed analog accelerators suffer from the intrinsic noise generated by their physical components, which makes it challenging to achieve high accuracy on deep neural networks. Hence, for successful deployment on analog accelerators, it is essential to be able to train deep neural networks to be robust to random continuous noise in the network weights, which is a somewhat new challenge in machine learning. In this paper, we advance the understanding of noisy neural networks. We outline how a noisy neural network has reduced learning capacity as a result of loss of mutual information between its input and output. To combat this, we propose using knowledge distillation combined with noise injection during training to achieve more noise robust networks, which is demonstrated experimentally across different networks and datasets, including ImageNet. Our method achieves models with as much as two times greater noise tolerance compared with the previous best attempts, which is a significant step towards making analog hardware practical for deep learning.},
  archiveprefix = {arXiv},
  eprint        = {2001.04974},
  file          = {:Zhou2020 - Noisy Machines_ Understanding Noisy Neural Networks and Enhancing Robustness to Analog Hardware Errors Using Distillation.pdf:PDF},
  keywords      = {noise, ann, analog, hardware, neuromorphic},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Wolfram1985,
  author     = {Stephen Wolfram},
  journal    = {Physical Review Letters},
  title      = {Undecidability and intractability in theoretical physics},
  year       = {1985},
  month      = {feb},
  number     = {8},
  pages      = {735--738},
  volume     = {54},
  doi        = {10.1103/physrevlett.54.735},
  file       = {:Wolfram1985 - Undecidability and Intractability in Theoretical Physics.pdf:PDF},
  keywords   = {undecid, dynsys, cir, physics},
  publisher  = {American Physical Society ({APS})},
  readstatus = {read},
}

@Article{Moore1996,
  author     = {Cristopher Moore},
  journal    = {Theoretical Computer Science},
  title      = {Recursion theory on the reals and continuous-time computation},
  year       = {1996},
  month      = {aug},
  number     = {1},
  pages      = {23--44},
  volume     = {162},
  doi        = {10.1016/0304-3975(95)00248-0},
  file       = {:Moore1996 - Recursion Theory on the Reals and Continuous Time Computation.pdf:PDF},
  keywords   = {analog, continuous, comp, theory},
  publisher  = {Elsevier {BV}},
  readstatus = {read},
}

@Article{Moore1990,
  author     = {Cristopher Moore},
  journal    = {Physical Review Letters},
  title      = {Unpredictability and undecidability in dynamical systems},
  year       = {1990},
  month      = {may},
  number     = {20},
  pages      = {2354--2357},
  volume     = {64},
  doi        = {10.1103/physrevlett.64.2354},
  file       = {:Moore1990 - Unpredictability and Undecidability in Dynamical Systems.pdf:PDF},
  keywords   = {dynsys, undecid, cir},
  publisher  = {American Physical Society ({APS})},
  readstatus = {read},
}

@Article{MacLennan2010,
  author     = {MacLennan, Bruce J},
  journal    = {International Journal of Unconventional Computing},
  title      = {The U-machine: A model of generalized computation.},
  year       = {2010},
  volume     = {6},
  file       = {:MacLennan2010 - The U Machine_ a Model of Generalized Computation..pdf:PDF},
  keywords   = {unconv, comp, theory},
  readstatus = {read},
}

@Article{Ball2004,
  author     = {Philip Ball},
  journal    = {Nature Materials},
  title      = {Designing with complexity},
  year       = {2004},
  month      = {feb},
  number     = {2},
  pages      = {78--78},
  volume     = {3},
  doi        = {10.1038/nmat1069},
  file       = {:Ball2004 - Designing with Complexity.pdf:PDF},
  keywords   = {complexity, views},
  publisher  = {Springer Science and Business Media {LLC}},
  readstatus = {read},
}

@Article{Bettoni1993,
  author     = {Marco C. Bettoni},
  journal    = {AI Communications},
  title      = {Made-Up Minds: A Constructivist Approach to Artificial Intelligence},
  year       = {1993},
  issn       = {09217126},
  pages      = {234-240},
  volume     = {6},
  doi        = {10.3233/AIC-1993-63-413},
  file       = {:Bettoni1993 - Made up Minds_ a Constructivist Approach to Artificial Intelligence.pdf:PDF},
  keywords   = {review, ai, cogn},
  publisher  = {IOS Press},
  readstatus = {read},
}

@Article{Bournez2018,
  author        = {Olivier Bournez and Amaury Pouly},
  title         = {A Survey on Analog Models of Computation},
  year          = {2018},
  month         = may,
  abstract      = {We present a survey on analog models of computations. Analog can be understood both as computing by analogy, or as working on the continuum. We consider both approaches, often intertwined, with a point of view mostly oriented by computation theory.},
  archiveprefix = {arXiv},
  eprint        = {1805.05729},
  file          = {:Bournez2018 - A Survey on Analog Models of Computation.pdf:PDF},
  keywords      = {comp, analog, survey},
  primaryclass  = {cs.CC},
  readstatus    = {read},
}

@Article{Crutchfield2010,
  author     = {James P. Crutchfield and William L. Ditto and Sudeshna Sinha},
  journal    = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  title      = {Introduction to Focus Issue: Intrinsic and Designed Computation: Information Processing in Dynamical Systems{\textemdash}Beyond the Digital Hegemony},
  year       = {2010},
  month      = {sep},
  number     = {3},
  pages      = {037101},
  volume     = {20},
  doi        = {10.1063/1.3492712},
  file       = {:Crutchfield2010 - Introduction to Focus Issue_ Intrinsic and Designed Computation_ Information Processing in Dynamical Systems_Beyond the Digital Hegemony.pdf:PDF},
  keywords   = {views, comp, dynsys, physics},
  publisher  = {{AIP} Publishing},
  readstatus = {read},
}

@Article{Endrullis2019,
  author     = {Jörg Endrullis and Jan Willem Klop and Rena Bakhshi},
  journal    = {Acta Informatica},
  title      = {Transducer degrees: atoms, infima and suprema},
  year       = {2019},
  month      = {dec},
  number     = {3-5},
  pages      = {727--758},
  volume     = {57},
  doi        = {10.1007/s00236-019-00353-7},
  file       = {:Endrullis2019 - Transducer Degrees_ Atoms, Infima and Suprema.pdf:PDF},
  keywords   = {comp, interact, theory},
  publisher  = {Springer Science and Business Media {LLC}},
  readstatus = {read},
}

@Article{Gardner1977,
  author     = {Martin Gardner},
  journal    = {Scientific American},
  title      = {Nonperiodic Tilings (MATHEMATICAL GAMES)},
  year       = {1977},
  issn       = {00368733, 19467087},
  number     = {1},
  pages      = {110--121},
  volume     = {236},
  file       = {:Gardner1977 - Nonperiodic Tilings (MATHEMATICAL GAMES).pdf:PDF},
  keywords   = {math, tiling},
  publisher  = {Scientific American, a division of Nature America, Inc.},
  readstatus = {read},
  url        = {http://www.jstor.org/stable/24953856},
}

@Article{Grzybowski2009,
  author     = {Bartosz A. Grzybowski and Christopher E. Wilmer and Jiwon Kim and Kevin P. Browne and Kyle J. M. Bishop},
  journal    = {Soft Matter},
  title      = {Self-assembly: from crystals to cells},
  year       = {2009},
  number     = {6},
  pages      = {1110},
  volume     = {5},
  doi        = {10.1039/b819321p},
  file       = {:Grzybowski2009 - Self Assembly_ from Crystals to Cells.pdf:PDF},
  keywords   = {sassembly, bio, chem, physics},
  publisher  = {Royal Society of Chemistry ({RSC})},
  readstatus = {read},
}

@Article{Guerin2011,
  author     = {Frank Guerin},
  journal    = {The Knowledge Engineering Review},
  title      = {Learning like a baby: a survey of artificial intelligence approaches},
  year       = {2011},
  month      = {may},
  number     = {2},
  pages      = {209--236},
  volume     = {26},
  doi        = {10.1017/s0269888911000038},
  file       = {:Guerin2011 - Learning like a Baby_ a Survey of Artificial Intelligence Approaches.pdf:PDF},
  keywords   = {ai, cogn, survey},
  publisher  = {Cambridge University Press ({CUP})},
  readstatus = {read},
}

@Article{Hens2019,
  author     = {Chittaranjan Hens and Uzi Harush and Simi Haber and Reuven Cohen and Baruch Barzel},
  journal    = {Nature Physics},
  title      = {Spatiotemporal signal propagation in complex networks},
  year       = {2019},
  month      = {jan},
  number     = {4},
  pages      = {403--412},
  volume     = {15},
  doi        = {10.1038/s41567-018-0409-0},
  file       = {:Hens2019 - Spatiotemporal Signal Propagation in Complex Networks.pdf:PDF;:Hens2019S - Spatiotemporal Signal Propagation in Complex Networks.pdf:PDF},
  keywords   = {signal, complexity, net},
  publisher  = {Springer Science and Business Media {LLC}},
  readstatus = {read},
}

@Article{Jaeger2004,
  author     = {H. Jaeger},
  journal    = {Science},
  title      = {Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communication},
  year       = {2004},
  month      = {apr},
  number     = {5667},
  pages      = {78--80},
  volume     = {304},
  doi        = {10.1126/science.1091277},
  file       = {:Jaeger2004 - Harnessing Nonlinearity_ Predicting Chaotic Systems and Saving Energy in Wireless Communication.pdf:PDF},
  keywords   = {reservoir, energy},
  publisher  = {American Association for the Advancement of Science ({AAAS})},
  readstatus = {read},
}

@InProceedings{Saade2016,
  author     = {A. Saade and F. Caltagirone and I. Carron and L. Daudet and A. Dremeau and S. Gigan and F. Krzakala},
  booktitle  = {2016 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
  title      = {Random projections through multiple optical scattering: Approximating Kernels at the speed of light},
  year       = {2016},
  month      = {mar},
  publisher  = {{IEEE}},
  doi        = {10.1109/icassp.2016.7472872},
  file       = {:Saade2016 - Random Projections through Multiple Optical Scattering_ Approximating Kernels at the Speed of Light.pdf:PDF},
  keywords   = {photonics, eml},
  readstatus = {read},
}

@InCollection{Lloyd2012,
  author     = {Seth Lloyd},
  booktitle  = {A Computable Universe},
  publisher  = {{WORLD} {SCIENTIFIC}},
  title      = {The Universe as Quantum Computer},
  year       = {2012},
  month      = {oct},
  pages      = {567--581},
  doi        = {10.1142/9789814374309_0029},
  file       = {:Lloyd2012 - The Universe As Quantum Computer.pdf:PDF},
  keywords   = {theory, phil, quantum, comp},
  readstatus = {read},
}

@Article{Saxe2020,
  author     = {Andrew Saxe and Stephanie Nelli and Christopher Summerfield},
  journal    = {Nature Reviews Neuroscience},
  title      = {If deep learning is the answer, what is the question?},
  year       = {2020},
  month      = {nov},
  number     = {1},
  pages      = {55--67},
  volume     = {22},
  doi        = {10.1038/s41583-020-00395-8},
  file       = {:Saxe2020 - If Deep Learning Is the Answer, What Is the Question_.pdf:PDF},
  keywords   = {ann, ai, cogn},
  publisher  = {Springer Science and Business Media {LLC}},
  readstatus = {read},
}

@Article{Semenova2019,
  author     = {N. Semenova and X. Porte and L. Andreoli and M. Jacquot and L. Larger and D. Brunner},
  journal    = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  title      = {Fundamental aspects of noise in analog-hardware neural networks},
  year       = {2019},
  month      = {oct},
  number     = {10},
  pages      = {103128},
  volume     = {29},
  doi        = {10.1063/1.5120824},
  file       = {:Semenova2019 - Fundamental Aspects of Noise in Analog Hardware Neural Networks.pdf:PDF},
  keywords   = {noise, analog, hardware, ann, snn, neuromorphic},
  publisher  = {{AIP} Publishing},
  readstatus = {read},
}

@Article{Tang2020,
  author     = {Yang Tang and Jürgen Kurths and Wei Lin and Edward Ott and Ljupco Kocarev},
  journal    = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  title      = {Introduction to Focus Issue: When machine learning meets complex systems: Networks, chaos, and nonlinear dynamics},
  year       = {2020},
  month      = {jun},
  number     = {6},
  pages      = {063151},
  volume     = {30},
  doi        = {10.1063/5.0016505},
  file       = {:Tang2020 - Introduction to Focus Issue_ When Machine Learning Meets Complex Systems_ Networks, Chaos, and Nonlinear Dynamics.pdf:PDF},
  keywords   = {views, ml, complexity, net, dynsys},
  publisher  = {{AIP} Publishing},
  readstatus = {read},
}

@Article{Zhang2020,
  author     = {Youhui Zhang and Peng Qu and Yu Ji and Weihao Zhang and Guangrong Gao and Guanrui Wang and Sen Song and Guoqi Li and Wenguang Chen and Weimin Zheng and Feng Chen and Jing Pei and Rong Zhao and Mingguo Zhao and Luping Shi},
  journal    = {Nature},
  title      = {A system hierarchy for brain-inspired computing},
  year       = {2020},
  month      = {oct},
  number     = {7829},
  pages      = {378--384},
  volume     = {586},
  doi        = {10.1038/s41586-020-2782-y},
  file       = {:Zhang2020 - A System Hierarchy for Brain Inspired Computing.pdf:PDF;:Zhang2020N - A System Hierarchy for Brain Inspired Computing.pdf:PDF;:Zhang2020S - A System Hierarchy for Brain Inspired Computing.pdf:PDF},
  keywords   = {neuromorphic, comp, theory},
  publisher  = {Springer Science and Business Media {LLC}},
  readstatus = {read},
}

@Article{Jaeger2014,
  author        = {Herbert Jaeger},
  title         = {Conceptors: an easy introduction},
  year          = {2014},
  month         = jun,
  abstract      = {Conceptors provide an elementary neuro-computational mechanism which sheds a fresh and unifying light on a diversity of cognitive phenomena. A number of demanding learning and processing tasks can be solved with unprecedented ease, robustness and accuracy. Some of these tasks were impossible to solve before. This entirely informal paper introduces the basic principles of conceptors and highlights some of their usages.},
  archiveprefix = {arXiv},
  eprint        = {1406.2671},
  file          = {:Jaeger2014 - Conceptors_ an Easy Introduction.pdf:PDF},
  keywords      = {conceptor, rnn},
  primaryclass  = {cs.NE},
  readstatus    = {read},
}

@Article{Jaeger2020,
  author        = {Herbert Jaeger},
  title         = {Exploring the landscapes of "computing": digital, neuromorphic, unconventional -- and beyond},
  year          = {2020},
  month         = nov,
  abstract      = {The acceleration race of digital computing technologies seems to be steering toward impasses -- technological, economical and environmental -- a condition that has spurred research efforts in alternative, "neuromorphic" (brain-like) computing technologies. Furthermore, since decades the idea of exploiting nonlinear physical phenomena "directly" for non-digital computing has been explored under names like "unconventional computing", "natural computing", "physical computing", or "in-materio computing". This has been taking place in niches which are small compared to other sectors of computer science. In this paper I stake out the grounds of how a general concept of "computing" can be developed which comprises digital, neuromorphic, unconventional and possible future "computing" paradigms. The main contribution of this paper is a wide-scope survey of existing formal conceptualizations of "computing". The survey inspects approaches rooted in three different kinds of background mathematics: discrete-symbolic formalisms, probabilistic modeling, and dynamical-systems oriented views. It turns out that different choices of background mathematics lead to decisively different understandings of what "computing" is. Across all of this diversity, a unifying coordinate system for theorizing about "computing" can be distilled. Within these coordinates I locate anchor points for a foundational formal theory of a future computing-engineering discipline that includes, but will reach beyond, digital and neuromorphic computing.},
  archiveprefix = {arXiv},
  eprint        = {2011.12013},
  file          = {:Jaeger2020 - Exploring the Landscapes of _computing__ Digital, Neuromorphic, Unconventional and beyond.pdf:PDF},
  keywords      = {comp, neuromorphic, unconv},
  primaryclass  = {cs.ET},
  readstatus    = {read},
}

@Article{Turing1950,
  author     = {Alan Mathison Turing},
  journal    = {Mind},
  title      = {Computing Machinery and Intelligence},
  year       = {1950},
  month      = oct,
  number     = {236},
  pages      = {433--460},
  volume     = {LIX},
  doi        = {10.1093/mind/lix.236.433},
  file       = {:Turing1950 - Computing Machinery and Intelligence.pdf:PDF},
  keywords   = {ai, comp, found, theory, phil},
  publisher  = {Oxford University Press ({OUP})},
  readstatus = {read},
}

@Misc{Turing1948,
  author     = {Turing, Alan Mathison},
  title      = {Intelligent machinery},
  year       = {1948},
  file       = {:Turing1948 - Intelligent Machinery.pdf:PDF},
  keywords   = {ai, comp, found, theory, phil},
  publisher  = {NPL. Mathematics Division},
  readstatus = {read},
}

@InCollection{Stepney2017,
  author     = {Susan Stepney},
  booktitle  = {Guide to Unconventional Computing for Music},
  publisher  = {Springer International Publishing},
  title      = {Introduction to Unconventional Computing},
  year       = {2017},
  pages      = {1--21},
  doi        = {10.1007/978-3-319-49881-2_1},
  file       = {:Stepney2017 - Introduction to Unconventional Computing.pdf:PDF},
  keywords   = {unconv, comp},
  readstatus = {read},
}

@Article{Eliasmith2012,
  author    = {C. Eliasmith and T. C. Stewart and X. Choo and T. Bekolay and T. DeWolf and Y. Tang and D. Rasmussen},
  journal   = {Science},
  title     = {A Large-Scale Model of the Functioning Brain},
  year      = {2012},
  month     = {nov},
  number    = {6111},
  pages     = {1202--1205},
  volume    = {338},
  doi       = {10.1126/science.1225266},
  file      = {:Eliasmith2012 - A Large Scale Model of the Functioning Brain.pdf:PDF},
  keywords  = {snn, neuroscience, neuromorphic, theory, found, dynsys, cogn},
  publisher = {American Association for the Advancement of Science ({AAAS})},
}

@Article{Tenenbaum2006,
  author    = {Joshua B. Tenenbaum and Thomas L. Griffiths and Charles Kemp},
  journal   = {Trends in Cognitive Sciences},
  title     = {Theory-based Bayesian models of inductive learning and reasoning},
  year      = {2006},
  month     = {jul},
  number    = {7},
  pages     = {309--318},
  volume    = {10},
  doi       = {10.1016/j.tics.2006.05.009},
  file      = {:Tenenbaum2006 - Theory Based Bayesian Models of Inductive Learning and Reasoning.pdf:PDF},
  keywords  = {views, survey, cogn, ai},
  publisher = {Elsevier {BV}},
}

@Article{Bueno2018,
  author    = {J. Bueno and S. Maktoobi and L. Froehly and I. Fischer and M. Jacquot and L. Larger and D. Brunner},
  journal   = {Optica},
  title     = {Reinforcement learning in a large-scale photonic recurrent neural network},
  year      = {2018},
  month     = {jun},
  number    = {6},
  pages     = {756},
  volume    = {5},
  doi       = {10.1364/optica.5.000756},
  file      = {:Bueno2018 - Reinforcement Learning in a Large Scale Photonic Recurrent Neural Network.pdf:PDF},
  keywords  = {rl, photonics, rnn},
  publisher = {The Optical Society},
}

@Article{Dambre2012,
  author    = {Joni Dambre and David Verstraeten and Benjamin Schrauwen and Serge Massar},
  journal   = {Scientific Reports},
  title     = {Information Processing Capacity of Dynamical Systems},
  year      = {2012},
  month     = {jul},
  number    = {1},
  volume    = {2},
  doi       = {10.1038/srep00514},
  file      = {:Dambre2012 - Information Processing Capacity of Dynamical Systems.pdf:PDF},
  keywords  = {dynsys, theory, comp},
  publisher = {Springer Science and Business Media {LLC}},
}

@InCollection{Lukosevicius2012,
  author     = {Mantas Luko{\v{s}}evi{\v{c}}ius},
  booktitle  = {Lecture Notes in Computer Science},
  publisher  = {Springer Berlin Heidelberg},
  title      = {A Practical Guide to Applying Echo State Networks},
  year       = {2012},
  pages      = {659--686},
  doi        = {10.1007/978-3-642-35289-8_36},
  file       = {:Lukosevicius2012 - A Practical Guide to Applying Echo State Networks.pdf:PDF},
  keywords   = {esn, reservoir, ann, rnn, alg},
  readstatus = {skimmed},
}

@Article{Sarpeshkar1998,
  author    = {Rahul Sarpeshkar},
  journal   = {Neural Computation},
  title     = {Analog Versus Digital: Extrapolating from Electronics to Neurobiology},
  year      = {1998},
  month     = {oct},
  number    = {7},
  pages     = {1601--1638},
  volume    = {10},
  doi       = {10.1162/089976698300017052},
  file      = {:Sarpeshkar1998 - Analog Versus Digital_ Extrapolating from Electronics to Neurobiology.pdf:PDF},
  keywords  = {analog, neuromorphic, neuroscience, bio, hardware},
  publisher = {{MIT} Press - Journals},
}

@Article{Balakrishnan2019,
  author     = {Harikrishnan Nellippallil Balakrishnan and Aditi Kathpalia and Snehanshu Saha and Nithin Nagaraj},
  journal    = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  title      = {{ChaosNet}: A chaos based artificial neural network architecture for classification},
  year       = {2019},
  month      = {nov},
  number     = {11},
  pages      = {113125},
  volume     = {29},
  doi        = {10.1063/1.5120831},
  file       = {:Balakrishnan2019 - ChaosNet_ a Chaos Based Artificial Neural Network Architecture for Classification.pdf:PDF},
  keywords   = {chaos, ann, noise},
  publisher  = {{AIP} Publishing},
  readstatus = {read},
}

@InProceedings{Mikolov2013,
  author    = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
  title     = {Distributed Representations of Words and Phrases and Their Compositionality},
  year      = {2013},
  address   = {Red Hook, NY, USA},
  pages     = {3111–3119},
  publisher = {Curran Associates Inc.},
  series    = {NIPS'13},
  abstract  = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
  file      = {:Mikolov2013 - Distributed Representations of Words and Phrases and Their Compositionality.pdf:PDF},
  keywords  = {nlp, hyperdim},
  location  = {Lake Tahoe, Nevada},
  numpages  = {9},
}

@Article{Tanaka2019,
  author     = {Gouhei Tanaka and Toshiyuki Yamane and Jean Benoit H{\'{e}}roux and Ryosho Nakane and Naoki Kanazawa and Seiji Takeda and Hidetoshi Numata and Daiju Nakano and Akira Hirose},
  journal    = {Neural Networks},
  title      = {Recent advances in physical reservoir computing: A review},
  year       = {2019},
  month      = {jul},
  pages      = {100--123},
  volume     = {115},
  doi        = {10.1016/j.neunet.2019.03.005},
  file       = {:Tanaka2019 - Recent Advances in Physical Reservoir Computing_ a Review.pdf:PDF},
  keywords   = {reservoir, survey, views, physics, materials},
  publisher  = {Elsevier {BV}},
  readstatus = {reading},
}

@Article{Siegelmann1995a,
  author     = {H.T. Siegelmann and E.D. Sontag},
  journal    = {Journal of Computer and System Sciences},
  title      = {On the Computational Power of Neural Nets},
  year       = {1995},
  month      = {feb},
  number     = {1},
  pages      = {132--150},
  volume     = {50},
  doi        = {10.1006/jcss.1995.1013},
  file       = {:Siegelmann1995a - On the Computational Power of Neural Nets.pdf:PDF},
  keywords   = {comp, ann, theory, complexity},
  publisher  = {Elsevier {BV}},
  readstatus = {read},
}

@Article{Siegelmann2003,
  author     = {Hava T. Siegelmann},
  journal    = {Minds and Machines},
  title      = {Neural and Super-Turing computing},
  year       = {2003},
  number     = {1},
  pages      = {103--114},
  volume     = {13},
  doi        = {10.1023/a:1021376718708},
  file       = {:Siegelmann2003 - Neural and Super Turing Computing.pdf:PDF},
  keywords   = {comp, ann, theory, complexity},
  publisher  = {Springer Science and Business Media {LLC}},
  readstatus = {read},
}

@Article{Siegelmann2013,
  author     = {Hava T. Siegelmann},
  journal    = {Progress in Biophysics and Molecular Biology},
  title      = {Turing on Super-Turing and adaptivity},
  year       = {2013},
  month      = {sep},
  number     = {1},
  pages      = {117--126},
  volume     = {113},
  doi        = {10.1016/j.pbiomolbio.2013.03.013},
  file       = {:Siegelmann2013 - Turing on Super Turing and Adaptivity.pdf:PDF},
  keywords   = {comp, ann, theory, complexity},
  publisher  = {Elsevier {BV}},
  readstatus = {read},
}

@Article{Clark2013,
  author    = {Andy Clark},
  journal   = {Behavioral and Brain Sciences},
  title     = {Whatever next? Predictive brains, situated agents, and the future of cognitive science},
  year      = {2013},
  month     = {may},
  number    = {3},
  pages     = {181--204},
  volume    = {36},
  doi       = {10.1017/s0140525x12000477},
  file      = {:Clark2013 - Whatever Next_ Predictive Brains, Situated Agents, and the Future of Cognitive Science.pdf:PDF},
  keywords  = {cogn, ai, views, survey},
  publisher = {Cambridge University Press ({CUP})},
}

@Article{Graves2014,
  author        = {Alex Graves and Greg Wayne and Ivo Danihelka},
  title         = {Neural Turing Machines},
  year          = {2014},
  month         = oct,
  abstract      = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
  archiveprefix = {arXiv},
  eprint        = {1410.5401},
  file          = {:Graves2014 - Neural Turing Machines.pdf:PDF},
  keywords      = {comp, ann, theory},
  primaryclass  = {cs.NE},
}

@Article{Schuman2017,
  author        = {Catherine D. Schuman and Thomas E. Potok and Robert M. Patton and J. Douglas Birdwell and Mark E. Dean and Garrett S. Rose and James S. Plank},
  title         = {A Survey of Neuromorphic Computing and Neural Networks in Hardware},
  year          = {2017},
  month         = may,
  abstract      = {Neuromorphic computing has come to refer to a variety of brain-inspired computers, devices, and models that contrast the pervasive von Neumann computer architecture. This biologically inspired approach has created highly connected synthetic neurons and synapses that can be used to model neuroscience theories as well as solve challenging machine learning problems. The promise of the technology is to create a brain-like ability to learn and adapt, but the technical challenges are significant, starting with an accurate neuroscience model of how the brain works, to finding materials and engineering breakthroughs to build devices to support these models, to creating a programming framework so the systems can learn, to creating applications with brain-like capabilities. In this work, we provide a comprehensive survey of the research and motivations for neuromorphic computing over its history. We begin with a 35-year review of the motivations and drivers of neuromorphic computing, then look at the major research areas of the field, which we define as neuro-inspired models, algorithms and learning approaches, hardware and devices, supporting systems, and finally applications. We conclude with a broad discussion on the major research topics that need to be addressed in the coming years to see the promise of neuromorphic computing fulfilled. The goals of this work are to provide an exhaustive review of the research conducted in neuromorphic computing since the inception of the term, and to motivate further work by illuminating gaps in the field where new research is needed.},
  archiveprefix = {arXiv},
  eprint        = {1705.06963},
  file          = {:Schuman2017 - A Survey of Neuromorphic Computing and Neural Networks in Hardware.pdf:PDF},
  keywords      = {survey, neuromorphic, hardware, ann, snn},
  primaryclass  = {cs.NE},
  readstatus    = {next},
  relevance     = {relevant},
}

@Article{Wolpert2015,
  author        = {David H. Wolpert},
  title         = {Extending Landauer's Bound from Bit Erasure to Arbitrary Computation},
  year          = {2015},
  month         = aug,
  abstract      = {Recent analyses have calculated the minimal thermodynamic work required to perform a computation pi when two conditions hold: the output of pi is independent of its input (e.g., as in bit erasure); we use a physical computer C to implement pi that is specially tailored to the environment of C, i.e., to the precise distribution over C's inputs, P_0. First I extend these analyses to calculate the work required even if the output of pi depends on its input, and even if C is not used with the distribution P_0 it was tailored for. Next I show that if C will be re-used, then the minimal work to run it depends only on the logical computation pi, independent of the physical details of C. This establishes a formal identity between the thermodynamics of (re-usable) computers and theoretical computer science. I use this identity to prove that the minimal work required to compute a bit string sigma on a "general purpose computer" rather than a special purpose one, i.e., on a universal Turing machine U, is k_BT ln(2) times the sum of three terms: The Kolmogorov complexity of sigma, log of the Bernoulli measure of the set of strings that compute sigma, and log of the halting probability of U. I also prove that using C with a distribution over environments results in an unavoidable increase in the work required to run the computer, even if it is tailored to the distribution over environments. I end by using these results to relate the free energy flux incident on an organism / robot / biosphere to the maximal amount of computation that the organism / robot / biosphere can do per unit time.},
  archiveprefix = {arXiv},
  eprint        = {1508.05319},
  file          = {:Wolpert2015 - Extending Landauer's Bound from Bit Erasure to Arbitrary Computation.pdf:PDF},
  keywords      = {stats, theory, comp, statph, thermo},
  primaryclass  = {cond-mat.stat-mech},
}

@Article{Hesslow2021,
  author        = {Daniel Hesslow and Iacopo Poli},
  title         = {Contrastive Embeddings for Neural Architectures},
  year          = {2021},
  month         = feb,
  abstract      = {The performance of algorithms for neural architecture search strongly depends on the parametrization of the search space. We use contrastive learning to identify networks across different initializations based on their data Jacobians, and automatically produce the first architecture embeddings independent from the parametrization of the search space. Using our contrastive embeddings, we show that traditional black-box optimization algorithms, without modification, can reach state-of-the-art performance in Neural Architecture Search. As our method provides a unified embedding space, we perform for the first time transfer learning between search spaces. Finally, we show the evolution of embeddings during training, motivating future studies into using embeddings at different training stages to gain a deeper understanding of the networks in a search space.},
  archiveprefix = {arXiv},
  eprint        = {2102.04208},
  file          = {:Hesslow2021 - Contrastive Embeddings for Neural Architectures.pdf:PDF},
  keywords      = {ml},
  primaryclass  = {cs.LG},
}

@Article{Whitesides2002,
  author    = {G. M. Whitesides, Bartosz Grzybowski},
  journal   = {Science},
  title     = {Self-Assembly at All Scales},
  year      = {2002},
  month     = {mar},
  number    = {5564},
  pages     = {2418--2421},
  volume    = {295},
  doi       = {10.1126/science.1070821},
  file      = {:Whitesides2002 - Self Assembly at All Scales.pdf:PDF},
  keywords  = {sassembly, chem, bio,},
  publisher = {American Association for the Advancement of Science ({AAAS})},
}

@InCollection{Bournez2008,
  author    = {Olivier Bournez and Manuel L. Campagnolo},
  booktitle = {New Computational Paradigms},
  publisher = {Springer New York},
  title     = {A Survey on Continuous Time Computations},
  year      = {2008},
  pages     = {383--423},
  doi       = {10.1007/978-0-387-68546-5_17},
  keywords  = {continuous, analog, survey, comp, theory},
}

@Article{Hopfield1982,
  author     = {J. J. Hopfield},
  journal    = {Proceedings of the National Academy of Sciences},
  title      = {Neural networks and physical systems with emergent collective computational abilities.},
  year       = {1982},
  month      = {apr},
  number     = {8},
  pages      = {2554--2558},
  volume     = {79},
  doi        = {10.1073/pnas.79.8.2554},
  file       = {:Hopfield1982 - Neural Networks and Physical Systems with Emergent Collective Computational Abilities..pdf:PDF},
  keywords   = {ann, physics, comp, found},
  publisher  = {Proceedings of the National Academy of Sciences},
  readstatus = {skimmed},
}

@Book{Neal1993,
  author    = {Neal, Radford M},
  publisher = {Department of Computer Science, University of Toronto Toronto, Ontario, Canada},
  title     = {Probabilistic inference using Markov chain Monte Carlo methods},
  year      = {1993},
  file      = {:Neal1993 - Probabilistic Inference Using Markov Chain Monte Carlo Methods.pdf:PDF},
  keywords  = {found, prob, stats},
}

@Article{Shannon1948,
  author     = {C. E. Shannon},
  journal    = {Bell System Technical Journal},
  title      = {A Mathematical Theory of Communication},
  year       = {1948},
  month      = {jul},
  number     = {3},
  pages      = {379--423},
  volume     = {27},
  doi        = {10.1002/j.1538-7305.1948.tb01338.x},
  file       = {:Shannon1948 - A Mathematical Theory of Communication.pdf:PDF},
  keywords   = {found, theory, math, info, signal},
  publisher  = {Institute of Electrical and Electronics Engineers ({IEEE})},
  readstatus = {reading},
}

@InProceedings{Aaronson2012,
  author     = {Scott Aaronson},
  booktitle  = {In Computability: Gödel, Turing, Church, and beyond (eds},
  title      = {Why philosophers should care about computational complexity},
  year       = {2012},
  publisher  = {MIT Press},
  file       = {:Aaronson2012 - Why Philosophers Should Care about Computational Complexity.pdf:PDF},
  keywords   = {comp, complexity, theory, phil},
  readstatus = {skimmed},
}

@InProceedings{Cabessa2011,
  author    = {Jeremie Cabessa and Hava T. Siegelmann},
  booktitle = {The 2011 International Joint Conference on Neural Networks},
  title     = {Evolving recurrent neural networks are super-Turing},
  year      = {2011},
  month     = {jul},
  publisher = {{IEEE}},
  doi       = {10.1109/ijcnn.2011.6033645},
  file      = {:Cabessa2011 - Evolving Recurrent Neural Networks Are Super Turing.pdf:PDF},
  keywords  = {evo, rnn, ann, comp, complexity},
}

@Article{Brown2020,
  author        = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  title         = {Language Models are Few-Shot Learners},
  year          = {2020},
  month         = may,
  abstract      = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arXiv},
  eprint        = {2005.14165},
  file          = {:Brown2020 - Language Models Are Few Shot Learners.pdf:PDF},
  keywords      = {nlp, demo},
  primaryclass  = {cs.CL},
  readstatus    = {skimmed},
}

@Article{Silver2018,
  author     = {David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and Laurent Sifre and Dharshan Kumaran and Thore Graepel and Timothy Lillicrap and Karen Simonyan and Demis Hassabis},
  journal    = {Science},
  title      = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
  year       = {2018},
  month      = {dec},
  number     = {6419},
  pages      = {1140--1144},
  volume     = {362},
  doi        = {10.1126/science.aar6404},
  file       = {:Silver2018 - A General Reinforcement Learning Algorithm That Masters Chess, Shogi, and Go through Self Play.pdf:PDF},
  keywords   = {rl, found, demo},
  publisher  = {American Association for the Advancement of Science ({AAAS})},
  readstatus = {skimmed},
}

@Article{Hettinger2017,
  author        = {Chris Hettinger and Tanner Christensen and Ben Ehlert and Jeffrey Humpherys and Tyler Jarvis and Sean Wade},
  title         = {Forward Thinking: Building and Training Neural Networks One Layer at a Time},
  year          = {2017},
  month         = jun,
  abstract      = {We present a general framework for training deep neural networks without backpropagation. This substantially decreases training time and also allows for construction of deep networks with many sorts of learners, including networks whose layers are defined by functions that are not easily differentiated, like decision trees. The main idea is that layers can be trained one at a time, and once they are trained, the input data are mapped forward through the layer to create a new learning problem. The process is repeated, transforming the data through multiple layers, one at a time, rendering a new data set, which is expected to be better behaved, and on which a final output layer can achieve good performance. We call this forward thinking and demonstrate a proof of concept by achieving state-of-the-art accuracy on the MNIST dataset for convolutional neural networks. We also provide a general mathematical formulation of forward thinking that allows for other types of deep learning problems to be considered.},
  archiveprefix = {arXiv},
  eprint        = {1706.02480},
  file          = {:Hettinger2017 - Forward Thinking_ Building and Training Neural Networks One Layer at a Time.pdf:PDF},
  keywords      = {ml, ann},
  primaryclass  = {stat.ML},
  readstatus    = {read},
}

@InProceedings{Bengio2006,
  author     = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
  booktitle  = {Proceedings of the 19th International Conference on Neural Information Processing Systems},
  title      = {Greedy Layer-Wise Training of Deep Networks},
  year       = {2006},
  address    = {Cambridge, MA, USA},
  pages      = {153–160},
  publisher  = {MIT Press},
  series     = {NIPS'06},
  abstract   = {Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.},
  file       = {:Bengio2006 - Greedy Layer Wise Training of Deep Networks.pdf:PDF},
  keywords   = {ann, ml, dl},
  location   = {Canada},
  numpages   = {8},
  readstatus = {skimmed},
}

@Article{Friston2010,
  author     = {Karl J. Friston and Jean Daunizeau and James Kilner and Stefan J. Kiebel},
  journal    = {Biological Cybernetics},
  title      = {Action and behavior: a free-energy formulation},
  year       = {2010},
  month      = {feb},
  number     = {3},
  pages      = {227--260},
  volume     = {102},
  doi        = {10.1007/s00422-010-0364-z},
  file       = {:Friston2010 - Action and Behavior_ a Free Energy Formulation.pdf:PDF},
  keywords   = {cogn, bio, theory, found},
  publisher  = {Springer Science and Business Media {LLC}},
  readstatus = {reading},
}

@Article{Jaeger2014a,
  author        = {Herbert Jaeger},
  title         = {Controlling Recurrent Neural Networks by Conceptors},
  year          = {2014},
  month         = mar,
  abstract      = {The human brain is a dynamical system whose extremely complex sensor-driven neural processes give rise to conceptual, logical cognition. Understanding the interplay between nonlinear neural dynamics and concept-level cognition remains a major scientific challenge. Here I propose a mechanism of neurodynamical organization, called conceptors, which unites nonlinear dynamics with basic principles of conceptual abstraction and logic. It becomes possible to learn, store, abstract, focus, morph, generalize, de-noise and recognize a large number of dynamical patterns within a single neural system; novel patterns can be added without interfering with previously acquired ones; neural noise is automatically filtered. Conceptors help explaining how conceptual-level information processing emerges naturally and robustly in neural systems, and remove a number of roadblocks in the theory and applications of recurrent neural networks.},
  archiveprefix = {arXiv},
  eprint        = {1403.3369},
  file          = {:Jaeger2014a - Controlling Recurrent Neural Networks by Conceptors.pdf:PDF},
  keywords      = {rnn, conceptor},
  primaryclass  = {cs.NE},
  readstatus    = {reading},
}

@Article{LeCun2015,
  author     = {Yann LeCun and Yoshua Bengio and Geoffrey Hinton},
  journal    = {Nature},
  title      = {Deep learning},
  year       = {2015},
  month      = {may},
  number     = {7553},
  pages      = {436--444},
  volume     = {521},
  doi        = {10.1038/nature14539},
  file       = {:LeCun2015 - Deep Learning.pdf:PDF},
  keywords   = {dl, ann},
  publisher  = {Springer Science and Business Media {LLC}},
  readstatus = {skimmed},
}

@Article{Sande2017,
  author     = {Guy Van der Sande and Daniel Brunner and Miguel C. Soriano},
  journal    = {Nanophotonics},
  title      = {Advances in photonic reservoir computing},
  year       = {2017},
  month      = {may},
  number     = {3},
  pages      = {561--576},
  volume     = {6},
  doi        = {10.1515/nanoph-2016-0132},
  file       = {:Sande2017 - Advances in Photonic Reservoir Computing.pdf:PDF},
  keywords   = {photonics, reservoir, comp, survey, views},
  publisher  = {Walter de Gruyter {GmbH}},
  readstatus = {next},
}

@Article{Vergis1986,
  author     = {Anastasios Vergis and Kenneth Steiglitz and Bradley Dickinson},
  journal    = {Mathematics and Computers in Simulation},
  title      = {The complexity of analog computation},
  year       = {1986},
  month      = {apr},
  number     = {2},
  pages      = {91--113},
  volume     = {28},
  doi        = {10.1016/0378-4754(86)90105-9},
  file       = {:Vergis1986 - The Complexity of Analog Computation.pdf:PDF},
  keywords   = {complexity, analog, comp, theory},
  publisher  = {Elsevier {BV}},
  readstatus = {next},
}

@Article{Aaronson2005,
  author     = {Aaronson, Scott},
  journal    = {SIGACT News},
  title      = {Guest Column: NP-Complete Problems and Physical Reality},
  year       = {2005},
  issn       = {0163-5700},
  month      = mar,
  number     = {1},
  pages      = {30–52},
  volume     = {36},
  abstract   = {Can NP-complete problems be solved efficiently in the physical universe? I survey proposals including soap bubbles, protein folding, quantum computing, quantum advice, quantum adiabatic algorithms, quantum-mechanical nonlinearities, hidden variables, relativistic time dilation, analog computing, Malament-Hogarth spacetimes, quantum gravity, closed timelike curves, and "anthropic computing." The section on soap bubbles even includes some "experimental" results. While I do not believe that any of the proposals will let us solve NP-complete problems efficiently, I argue that by studying them, we can learn something not only about computation but also about physics.},
  address    = {New York, NY, USA},
  doi        = {10.1145/1052796.1052804},
  file       = {:Aaronson2005 - Guest Column_ NP Complete Problems and Physical Reality.pdf:PDF},
  issue_date = {March 2005},
  keywords   = {phil, comp, complexity, physics},
  numpages   = {23},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
}

@PhdThesis{Verbaan2005,
  author     = {Verbaan, Peter},
  school     = {Utrecht University},
  title      = {The computational complexity of evolving systems},
  year       = {2005},
  address    = {Eindhoven},
  file       = {:Verbaan2005 - The Computational Complexity of Evolving Systems.pdf:PDF},
  isbn       = {9039341559},
  keywords   = {evo, comp, complexity, theory, interact},
  publisher  = {Research school IPA (Institute for Programming research and Algorithmics},
  readstatus = {skimmed},
}

@PhdThesis{Lawrence2018,
  author    = {Lawrence, {Celestine Preetham}},
  school    = {UT},
  title     = {Evolving Networks To Have Intelligence Realized At Nanoscale},
  year      = {2018},
  address   = {Netherlands},
  month     = may,
  note      = {IDS PhD thesis series no. 18-464 We acknowledge financial support from the MESA+ Institute for Nanotechnology and the CTIT Institute for ICT research, as well as the European Community{\textquoteright}s Seventh Framework Programme (FP7/2007–2013) under grant agreement No. 317662.},
  abstract  = {In this thesis, we have made the following progress.Introducing {\textquoteleft}neuroevolution in nanomaterio{\textquoteright} as a method to reach ultimate physical limits of computing. A mathematically compact treatment of SET networks that enables analytical computation of stability diagrams. A novel mean-field approximation to simulate large scale SET networks. Illustrative simulations that show the rich physics of SET networks that are exploitable for logic, memory and pattern recognition. The first experimental demonstration of Boolean logic functionality in a disordered system at the nanoscale. Conception of the NP internet architecture for neuroevolution, and an illustration of it implementing classification of binary images. A definition of intelligence and an IQ metric, demonstrating its applicability on a nanomaterial cluster of dopant atoms.},
  day       = {16},
  doi       = {10.3990/1.9789036545471},
  file      = {:Lawrence2018 - Evolving Networks to Have Intelligence Realized at Nanoscale.pdf:PDF},
  isbn      = {978-90-365-4547-1},
  keywords  = {evo, nano, net, ai},
  language  = {English},
  publisher = {University of Twente},
}

@PhdThesis{Pouly2015,
  author      = {Pouly, Amaury},
  school      = {{Ecole Doctorale Polytechnique ; Universidad do Algarve}},
  title       = {Continuous models of computation: from computability to complexity},
  year        = {2015},
  month       = Jul,
  type        = {Theses},
  file        = {:Pouly2015 - Continuous Models of Computation_ from Computability to Complexity.pdf:PDF},
  hal_id      = {tel-01223284},
  hal_version = {v2},
  keywords    = {continuous, analog, comp, complexity, theory, survey},
  pdf         = {https://pastel.archives-ouvertes.fr/tel-01223284v2/file/thesis.pdf},
  url         = {https://pastel.archives-ouvertes.fr/tel-01223284},
}

@Book{Bach2009,
  author    = {Bach, Joscha},
  publisher = {Oxford University Press},
  title     = {Principles of synthetic intelligence : PSI : an architecture of motivated cognition},
  year      = {2009},
  address   = {Oxford New York},
  isbn      = {9780195370676},
  file      = {:Bach2009 - Principles of Synthetic Intelligence _ PSI _ an Architecture of Motivated Cognition.pdf:PDF},
  keywords  = {cogn, ai, theory},
}

@Article{Bennett1985,
  author     = {Charles H. Bennett and Rolf Landauer},
  journal    = {Scientific American},
  title      = {The Fundamental Physical Limits of Computation},
  year       = {1985},
  issn       = {00368733, 19467087},
  number     = {1},
  pages      = {48--57},
  volume     = {253},
  file       = {:Bennett1985 - The Fundamental Physical Limits of Computation.pdf:PDF},
  keywords   = {found, comp, physics, thermo, statph},
  publisher  = {Scientific American, a division of Nature America, Inc.},
  readstatus = {next},
  url        = {http://www.jstor.org/stable/24967723},
}

@Article{Duport2016,
  author     = {Fran{\c{c}}ois Duport and Anteo Smerieri and Akram Akrout and Marc Haelterman and Serge Massar},
  journal    = {Scientific Reports},
  title      = {Fully analogue photonic reservoir computer},
  year       = {2016},
  month      = {mar},
  number     = {1},
  volume     = {6},
  doi        = {10.1038/srep22381},
  file       = {:Duport2016 - Fully Analogue Photonic Reservoir Computer.pdf:PDF},
  keywords   = {analog, photonics, reservoir, comp},
  publisher  = {Springer Science and Business Media {LLC}},
  readstatus = {next},
}

@Article{Markov2014,
  author     = {Igor L. Markov},
  journal    = {Nature},
  title      = {Limits on fundamental limits to computation},
  year       = {2014},
  month      = {aug},
  number     = {7513},
  pages      = {147--154},
  volume     = {512},
  doi        = {10.1038/nature13570},
  keywords   = {comp, complexity, physics, statph, thermo},
  publisher  = {Springer Science and Business Media {LLC}},
  readstatus = {next},
}

@Article{Lloyd2000,
  author    = {Seth Lloyd},
  journal   = {Nature},
  title     = {Ultimate physical limits to computation},
  year      = {2000},
  month     = {aug},
  number    = {6799},
  pages     = {1047--1054},
  volume    = {406},
  doi       = {10.1038/35023282},
  keywords  = {comp, complexity, physics, statph, thermo},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{West1999,
  author     = {G. B. West},
  journal    = {Science},
  title      = {The Fourth Dimension of Life: Fractal Geometry and Allometric Scaling of Organisms},
  year       = {1999},
  month      = {jun},
  number     = {5420},
  pages      = {1677--1679},
  volume     = {284},
  doi        = {10.1126/science.284.5420.1677},
  file       = {:West1999 - The Fourth Dimension of Life_ Fractal Geometry and Allometric Scaling of Organisms.pdf:PDF},
  keywords   = {math, bio, found},
  publisher  = {American Association for the Advancement of Science ({AAAS})},
  readstatus = {read},
}

@Article{Turing1952,
  author     = {Alan Mathison Turing},
  journal    = {Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences},
  title      = {The chemical basis of morphogenesis},
  year       = {1952},
  month      = {aug},
  number     = {641},
  pages      = {37--72},
  volume     = {237},
  doi        = {10.1098/rstb.1952.0012},
  file       = {:Turing1952 - The Chemical Basis of Morphogenesis.pdf:PDF},
  keywords   = {bio, chem, found, cogn, ai, comp},
  publisher  = {The Royal Society},
  readstatus = {next},
}

@Article{Vandoorne2014,
  author     = {Kristof Vandoorne and Pauline Mechet and Thomas Van Vaerenbergh and Martin Fiers and Geert Morthier and David Verstraeten and Benjamin Schrauwen and Joni Dambre and Peter Bienstman},
  journal    = {Nature Communications},
  title      = {Experimental demonstration of reservoir computing on a silicon photonics chip},
  year       = {2014},
  month      = {mar},
  number     = {1},
  volume     = {5},
  doi        = {10.1038/ncomms4541},
  file       = {:Vandoorne2014 - Experimental Demonstration of Reservoir Computing on a Silicon Photonics Chip.pdf:PDF},
  keywords   = {demo, reservoir, photonics, comp},
  publisher  = {Springer Science and Business Media {LLC}},
  readstatus = {next},
}

@InCollection{Sloman2002,
  author     = {Aaron Sloman},
  booktitle  = {Emotions in Humans and Artifacts},
  publisher  = {Bradford Book/Mit Press},
  title      = {How Many Separately Evolved Emotional Beasties Live Within Us},
  year       = {2002},
  editor     = {Robert Trappl},
  pages      = {35--114},
  file       = {:Sloman2002 - How Many Separately Evolved Emotional Beasties Live within Us.pdf:PDF},
  keywords   = {cogn, ai, evo, phil},
  readstatus = {next},
}

@Article{Landauer1996,
  author     = {Rolf Landauer},
  journal    = {Physics Letters A},
  title      = {The physical nature of information},
  year       = {1996},
  month      = {jul},
  number     = {4-5},
  pages      = {188--193},
  volume     = {217},
  doi        = {10.1016/0375-9601(96)00453-7},
  file       = {:Landauer1996 - The Physical Nature of Information.pdf:PDF},
  keywords   = {physics, info, comp, statph, stats},
  publisher  = {Elsevier {BV}},
  readstatus = {next},
}

@InCollection{Leeuwen2001a,
  author     = {Jan van Leeuwen and Ji{\v{r}}{\'{\i}} Wiedermann},
  booktitle  = {Mathematics Unlimited {\textemdash} 2001 and Beyond},
  publisher  = {Springer Berlin Heidelberg},
  title      = {The Turing Machine Paradigm in Contemporary Computing},
  year       = {2001},
  pages      = {1139--1155},
  doi        = {10.1007/978-3-642-56478-9_59},
  file       = {:Leeuwen2001a - The Turing Machine Paradigm in Contemporary Computing.pdf:PDF},
  keywords   = {interact, comp, complexity, theory},
  readstatus = {skimmed},
}

@Article{Izhikevich2003,
  author     = {E.M. Izhikevich},
  journal    = {{IEEE} Transactions on Neural Networks},
  title      = {Simple model of spiking neurons},
  year       = {2003},
  month      = {nov},
  number     = {6},
  pages      = {1569--1572},
  volume     = {14},
  doi        = {10.1109/tnn.2003.820440},
  file       = {:Izhikevich2003 - Simple Model of Spiking Neurons.pdf:PDF},
  keywords   = {neuroscience, snn},
  publisher  = {Institute of Electrical and Electronics Engineers ({IEEE})},
  readstatus = {skimmed},
}

@InProceedings{Sculley2014,
  author     = {D. Sculley and Gary Holt and Daniel Golovin and Eugene Davydov and Todd Phillips and Dietmar Ebner and Vinay Chaudhary and Michael Young},
  booktitle  = {SE4ML: Software Engineering for Machine Learning (NIPS 2014 Workshop)},
  title      = {Machine Learning: The High Interest Credit Card of Technical Debt},
  year       = {2014},
  file       = {:Sculley2014 - Machine Learning_ the High Interest Credit Card of Technical Debt.pdf:PDF},
  keywords   = {ml, phil},
  readstatus = {skimmed},
}

@Article{Graves2016,
  author     = {Alex Graves and Greg Wayne and Malcolm Reynolds and Tim Harley and Ivo Danihelka and Agnieszka Grabska-Barwi{\'{n}}ska and Sergio G{\'{o}}mez Colmenarejo and Edward Grefenstette and Tiago Ramalho and John Agapiou and Adri{\`{a}} Puigdom{\`{e}}nech Badia and Karl Moritz Hermann and Yori Zwols and Georg Ostrovski and Adam Cain and Helen King and Christopher Summerfield and Phil Blunsom and Koray Kavukcuoglu and Demis Hassabis},
  journal    = {Nature},
  title      = {Hybrid computing using a neural network with dynamic external memory},
  year       = {2016},
  month      = {oct},
  number     = {7626},
  pages      = {471--476},
  volume     = {538},
  doi        = {10.1038/nature20101},
  file       = {:Graves2016 - Hybrid Computing Using a Neural Network with Dynamic External Memory.pdf:PDF},
  keywords   = {rl, comp, ann, dynsys, theory, progr},
  publisher  = {Springer Science and Business Media {LLC}},
  readstatus = {next},
}

@Article{Beal2015,
  author     = {Jacob Beal and Mirko Viroli},
  journal    = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  title      = {Space{\textendash}time programming},
  year       = {2015},
  month      = {jul},
  number     = {2046},
  pages      = {20140220},
  volume     = {373},
  doi        = {10.1098/rsta.2014.0220},
  file       = {:Beal2015 - Space_time Programming.pdf:PDF},
  keywords   = {progr, theory, physics, comp},
  publisher  = {The Royal Society},
  readstatus = {next},
}

@Article{Neckar2019,
  author     = {Alexander Neckar and Sam Fok and Ben V. Benjamin and Terrence C. Stewart and Nick N. Oza and Aaron R. Voelker and Chris Eliasmith and Rajit Manohar and Kwabena Boahen},
  journal    = {Proceedings of the {IEEE}},
  title      = {Braindrop: A Mixed-Signal Neuromorphic Architecture With a Dynamical Systems-Based Programming Model},
  year       = {2019},
  month      = {jan},
  number     = {1},
  pages      = {144--164},
  volume     = {107},
  doi        = {10.1109/jproc.2018.2881432},
  file       = {:Neckar2019 - Braindrop_ a Mixed Signal Neuromorphic Architecture with a Dynamical Systems Based Programming Model.pdf:PDF},
  keywords   = {neuromorphic, hardware, dynsys, progr, comp, signal},
  publisher  = {Institute of Electrical and Electronics Engineers ({IEEE})},
  readstatus = {next},
}

@Article{Hawkins2019,
  author    = {Jeff Hawkins and Marcus Lewis and Mirko Klukas and Scott Purdy and Subutai Ahmad},
  journal   = {Frontiers in Neural Circuits},
  title     = {A Framework for Intelligence and Cortical Function Based on Grid Cells in the Neocortex},
  year      = {2019},
  month     = {jan},
  volume    = {12},
  doi       = {10.3389/fncir.2018.00121},
  file      = {:Hawkins2019 - A Framework for Intelligence and Cortical Function Based on Grid Cells in the Neocortex.pdf:PDF},
  keywords  = {cogn, neuroscience, ai},
  publisher = {Frontiers Media {SA}},
}

@Article{Brooks1991,
  author     = {Rodney A. Brooks},
  journal    = {Artificial Intelligence},
  title      = {Intelligence without representation},
  year       = {1991},
  month      = {jan},
  number     = {1-3},
  pages      = {139--159},
  volume     = {47},
  doi        = {10.1016/0004-3702(91)90053-m},
  keywords   = {ai, cogn, found, views},
  publisher  = {Elsevier {BV}},
  readstatus = {next},
}

@InProceedings{Brooks1991a,
  author     = {Brooks, Rodney A.},
  booktitle  = {Proceedings of the 12th International Joint Conference on Artificial Intelligence - Volume 1},
  title      = {Intelligence without Reason},
  year       = {1991},
  address    = {San Francisco, CA, USA},
  pages      = {569–595},
  publisher  = {Morgan Kaufmann Publishers Inc.},
  series     = {IJCAI'91},
  abstract   = {Computers and Thought are the two categories that together define Artificial Intelligence as a discipline. It is generally accepted that work in Artificial Intelligence over the last thirty years has had a strong influence on aspects of computer architectures. In this paper we also make the converse claim; that the state of computer architecture has been a strong influence on our models of thought. The Von Neumann model of computation has lead Artificial Intelligence in particular directions. Intelligence in biological systems is completely different. Recent work in behavior-based Artificial Intelligence has produced new models of intelligence that are much closer in spirit to biological systems. The non-Von Neumann computational models they use share many characteristics with biological computation.},
  file       = {:Brooks1991a - Intelligence without Reason.pdf:PDF},
  isbn       = {1558601600},
  keywords   = {ai, cogn, phil, views},
  location   = {Sydney, New South Wales, Australia},
  numpages   = {27},
  readstatus = {next},
}

@Article{Basye1995,
  author     = {Kenneth Basye and Thomas Dean and Leslie Pack Kaelbling},
  journal    = {Artificial Intelligence},
  title      = {Learning dynamics: system identification for perceptually challenged agents},
  year       = {1995},
  month      = {jan},
  number     = {1-2},
  pages      = {139--171},
  volume     = {72},
  doi        = {10.1016/0004-3702(94)00023-t},
  file       = {:Basye1995 - Learning Dynamics_ System Identification for Perceptually Challenged Agents.pdf:PDF},
  keywords   = {dynsys, ml, ai},
  publisher  = {Elsevier {BV}},
  readstatus = {next},
}

@Article{Chen2016,
  author     = {Ying Chen and JD Elenee Argentinis and Griff Weber},
  journal    = {Clinical Therapeutics},
  title      = {{IBM} Watson: How Cognitive Computing Can Be Applied to Big Data Challenges in Life Sciences Research},
  year       = {2016},
  month      = {apr},
  number     = {4},
  pages      = {688--701},
  volume     = {38},
  doi        = {10.1016/j.clinthera.2015.12.001},
  file       = {:Chen2016 - IBM Watson_ How Cognitive Computing Can Be Applied to Big Data Challenges in Life Sciences Research.pdf:PDF},
  keywords   = {ai, ml, cogn, comp, bio, chem},
  publisher  = {Elsevier {BV}},
  readstatus = {next},
}

@Article{Mitchell2002,
  author     = {Melanie Mitchell},
  journal    = {Science},
  title      = {Is the Universe a Universal Computer?},
  year       = {2002},
  month      = {oct},
  number     = {5591},
  pages      = {65--68},
  volume     = {298},
  doi        = {10.1126/science.1075073},
  file       = {:Mitchell2002 - Is the Universe a Universal Computer_.pdf:PDF},
  keywords   = {phil, views, review, comp, physics},
  publisher  = {American Association for the Advancement of Science ({AAAS})},
  readstatus = {next},
}

@Article{Jaeger2016,
  author     = {Herbert Jaeger},
  journal    = {Nature},
  title      = {Deep neural reasoning},
  year       = {2016},
  month      = {oct},
  number     = {7626},
  pages      = {467--468},
  volume     = {538},
  doi        = {10.1038/nature19477},
  file       = {:Jaeger2016 - Deep Neural Reasoning.pdf:PDF},
  keywords   = {neuroscience, ann, review, views},
  publisher  = {Springer Science and Business Media {LLC}},
  readstatus = {next},
}

@Article{Siegelmann1996,
  author     = {Hava T. Siegelmann},
  journal    = {Theoretical Computer Science},
  title      = {The simple dynamics of super Turing theories},
  year       = {1996},
  month      = {nov},
  number     = {2},
  pages      = {461--472},
  volume     = {168},
  doi        = {10.1016/s0304-3975(96)00087-4},
  file       = {:Siegelmann1996 - The Simple Dynamics of Super Turing Theories.pdf:PDF},
  keywords   = {dynsys, comp, complexity, theory},
  publisher  = {Elsevier {BV}},
  readstatus = {skimmed},
}

@Article{Neumann1956,
  author     = {John von Neumann},
  journal    = {Automata Studies},
  title      = {Probabilistic logics and the synthesis of reliable organisms from unreliable components},
  year       = {1956},
  pages      = {43--98},
  file       = {:Neumann1956 - Probabilistic Logics and the Synthesis of Reliable Organisms from Unreliable Components.pdf:PDF},
  keywords   = {prob, logic, theory, found, comp},
  publisher  = {Princeton University Press},
  readstatus = {next},
}

@Article{Paquot2012,
  author     = {Y. Paquot and F. Duport and A. Smerieri and J. Dambre and B. Schrauwen and M. Haelterman and S. Massar},
  journal    = {Scientific Reports},
  title      = {Optoelectronic Reservoir Computing},
  year       = {2012},
  month      = {feb},
  number     = {1},
  volume     = {2},
  doi        = {10.1038/srep00287},
  file       = {:Paquot2012 - Optoelectronic Reservoir Computing.pdf:PDF},
  keywords   = {photonics, hardware, comp, reservoir},
  publisher  = {Springer Science and Business Media {LLC}},
  readstatus = {next},
}

@Article{Lake2016,
  author     = {Brenden M. Lake and Tomer D. Ullman and Joshua B. Tenenbaum and Samuel J. Gershman},
  journal    = {Behavioral and Brain Sciences},
  title      = {Building machines that learn and think like people},
  year       = {2016},
  month      = {nov},
  volume     = {40},
  doi        = {10.1017/s0140525x16001837},
  file       = {:Lake2016 - Building Machines That Learn and Think like People.pdf:PDF},
  keywords   = {cogn, ai, views, phil},
  publisher  = {Cambridge University Press ({CUP})},
  readstatus = {next},
}

@InBook{PaugamMoisy2012,
  author     = {Paugam-Moisy, H. and Bohte, Sander},
  publisher  = {Springer Verlag},
  title      = {Computing with Spiking Neuron Networks},
  year       = {2012},
  month      = jan,
  booktitle  = {Handbook of Natural Computing},
  file       = {:PaugamMoisy2012 - Computing with Spiking Neuron Networks.pdf:PDF},
  keywords   = {snn, comp, neuromorphic},
  readstatus = {next},
}

@Book{Rozenberg2012,
  author    = {Rozenberg, Grzegorz},
  publisher = {Springer},
  title     = {Handbook of natural computing},
  year      = {2012},
  address   = {Berlin London},
  isbn      = {9783540929109},
  keywords  = {unconv, comp, neuromorphic, theory},
}

@Book{Savage1998,
  author     = {Savage, John E},
  publisher  = {Addison-Wesley Reading, MA},
  title      = {Models of computation},
  year       = {1998},
  volume     = {136},
  file       = {:Savage1998 - Models of Computation.pdf:PDF},
  keywords   = {comp, theory},
  readstatus = {next},
}

@Book{Neumann2012,
  author     = {von Neumann, John and Kurzweil, Ray},
  publisher  = {Yale University Press},
  title      = {The computer and the brain},
  year       = {2012},
  file       = {:Neumann2012 - The Computer and the Brain.pdf:PDF},
  keywords   = {comp, neuroscience, neuromorphic, snn},
  readstatus = {read},
}

@Book{Perumalla2013,
  author     = {Perumalla, Kalyan S},
  publisher  = {CRC Press},
  title      = {Introduction to reversible computing},
  year       = {2013},
  file       = {:Perumalla2013 - Introduction to Reversible Computing.pdf:PDF},
  keywords   = {reversible, comp, theory},
  readstatus = {next},
}

@InProceedings{Kwisthout2020,
  author     = {Johan Kwisthout and Nils Donselaar},
  booktitle  = {Proceedings of the Neuro-inspired Computational Elements Workshop},
  title      = {On the computational power and complexity of Spiking Neural Networks},
  year       = {2020},
  month      = {mar},
  publisher  = {{ACM}},
  doi        = {10.1145/3381755.3381760},
  file       = {:Kwisthout2020 - On the Computational Power and Complexity of Spiking Neural Networks.pdf:PDF},
  keywords   = {complexity, neuromorphic, snn},
  readstatus = {next},
}

@InProceedings{Roy2013,
  author     = {Swapnoneel Roy and Atri Rudra and Akshat Verma},
  booktitle  = {Proceedings of the 4th conference on Innovations in Theoretical Computer Science - {ITCS} {\textquotesingle}13},
  title      = {An energy complexity model for algorithms},
  year       = {2013},
  publisher  = {{ACM} Press},
  doi        = {10.1145/2422436.2422470},
  file       = {:Roy2013 - An Energy Complexity Model for Algorithms.pdf:PDF},
  keywords   = {energy, complexity, alg, comp},
  readstatus = {next},
}

@Article{Martin2001,
  author     = {Alain J. Martin},
  journal    = {Information Processing Letters},
  title      = {Towards an energy complexity of computation},
  year       = {2001},
  month      = {feb},
  number     = {2-4},
  pages      = {181--187},
  volume     = {77},
  doi        = {10.1016/s0020-0190(00)00214-3},
  file       = {:Martin2001 - Towards an Energy Complexity of Computation.pdf:PDF},
  keywords   = {energy, complexity, alg, comp},
  publisher  = {Elsevier {BV}},
  readstatus = {next},
}

@Article{Manin2014,
  author     = {Y I Manin},
  journal    = {Journal of Physics: Conference Series},
  title      = {Complexity vs energy: theory of computation and theoretical physics},
  year       = {2014},
  month      = {sep},
  pages      = {012018},
  volume     = {532},
  doi        = {10.1088/1742-6596/532/1/012018},
  file       = {:Manin2014 - Complexity Vs Energy_ Theory of Computation and Theoretical Physics.pdf:PDF},
  keywords   = {energy, complexity, alg, comp},
  publisher  = {{IOP} Publishing},
  readstatus = {next},
}

@InProceedings{Jain2005,
  author       = {R. Jain and D. Molnar and Z. Ramzan},
  booktitle    = {{IEEE} Wireless Communications and Networking Conference, 2005},
  title        = {Towards a model of energy complexity for algorithms},
  year         = {2005},
  organization = {IEEE},
  pages        = {1884--1890},
  publisher    = {{IEEE}},
  volume       = {3},
  doi          = {10.1109/wcnc.2005.1424799},
  file         = {:Jain2005 - Towards a Model of Energy Complexity for Algorithms.pdf:PDF},
  keywords     = {energy, complexity, alg, comp},
  readstatus   = {next},
}

@Article{Varela1974,
  author     = {Varela, F. G. and Maturana, H. R. and Uribe, R.},
  journal    = {Biosystems},
  title      = {Autopoiesis: The organization of living systems, its characterization and a model},
  year       = {1974},
  number     = {4},
  pages      = {187-196},
  volume     = {5},
  file       = {:Varela1974 - Autopoiesis_ the Organization of Living Systems, Its Characterization and a Model.pdf:PDF},
  keywords   = {bio, phil, found, selforg},
  readstatus = {read},
}

@Article{Bartolozzi2007,
  author     = {Chiara Bartolozzi and Giacomo Indiveri},
  journal    = {Neural Computation},
  title      = {Synaptic Dynamics in Analog {VLSI}},
  year       = {2007},
  month      = {oct},
  number     = {10},
  pages      = {2581--2603},
  volume     = {19},
  doi        = {10.1162/neco.2007.19.10.2581},
  file       = {:Bartolozzi2007 - Synaptic Dynamics in Analog VLSI.pdf:PDF},
  keywords   = {analog, hardware, dynsys, neuroscience, neuromorphic},
  priority   = {prio1},
  publisher  = {{MIT} Press - Journals},
  readstatus = {next},
}

@Article{Indiveri2006,
  author    = {G. Indiveri and E. Chicca and R. Douglas},
  journal   = {{IEEE} Transactions on Neural Networks},
  title     = {A {VLSI} Array of Low-Power Spiking Neurons and Bistable Synapses With Spike-Timing Dependent Plasticity},
  year      = {2006},
  month     = {jan},
  number    = {1},
  pages     = {211--221},
  volume    = {17},
  doi       = {10.1109/tnn.2005.860850},
  file      = {:Indiveri2006 - A VLSI Array of Low Power Spiking Neurons and Bistable Synapses with Spike Timing Dependent Plasticity.pdf:PDF},
  keywords  = {neuromorphic, hardware, snn},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@InCollection{Choudhary2012,
  author     = {Swadesh Choudhary and Steven Sloan and Sam Fok and Alexander Neckar and Eric Trautmann and Peiran Gao and Terry Stewart and Chris Eliasmith and Kwabena Boahen},
  booktitle  = {Artificial Neural Networks and Machine Learning {\textendash} {ICANN} 2012},
  publisher  = {Springer Berlin Heidelberg},
  title      = {Silicon Neurons That Compute},
  year       = {2012},
  pages      = {121--128},
  doi        = {10.1007/978-3-642-33269-2_16},
  file       = {:Choudhary2012 - Silicon Neurons That Compute.pdf:PDF},
  keywords   = {snn, hardware, neuromorphic},
  priority   = {prio1},
  readstatus = {next},
}

@Article{Silver2007,
  author    = {R. Silver and K. Boahen and S. Grillner and N. Kopell and K. L. Olsen},
  journal   = {Journal of Neuroscience},
  title     = {Neurotech for Neuroscience: Unifying Concepts, Organizing Principles, and Emerging Tools},
  year      = {2007},
  month     = {oct},
  number    = {44},
  pages     = {11807--11819},
  volume    = {27},
  doi       = {10.1523/jneurosci.3575-07.2007},
  file      = {:Silver2007 - Neurotech for Neuroscience_ Unifying Concepts, Organizing Principles, and Emerging Tools.pdf:PDF},
  keywords  = {neuroscience, neuromorphic, concepts, review, survey},
  publisher = {Society for Neuroscience},
}

@Article{Mead1989,
  author   = {Mead, Carver},
  journal  = {NASA STI/Recon Technical Report A},
  title    = {{Analog VLSI and neutral systems}},
  year     = {1989},
  month    = jan,
  pages    = {16574},
  volume   = {90},
  adsnote  = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl   = {https://ui.adsabs.harvard.edu/abs/1989STIA...9016574M},
  keywords = {analog, hardware, neuromorphic},
}

@Article{Mead1990,
  author    = {Carver Mead},
  journal   = {Proceedings of the {IEEE}},
  title     = {Neuromorphic electronic systems},
  year      = {1990},
  number    = {10},
  pages     = {1629--1636},
  volume    = {78},
  doi       = {10.1109/5.58356},
  file      = {:Mead1990 - Neuromorphic Electronic Systems.pdf:PDF},
  keywords  = {neuromorphic, hardware, found},
  priority  = {prio1},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Jaeger2021,
  author    = {Herbert Jaeger},
  journal   = {Neuromorphic Computing and Engineering},
  title     = {Toward a generalized theory comprising digital, neuromorphic, and unconventional computing},
  year      = {2021},
  month     = {mar},
  doi       = {10.1088/2634-4386/abf151},
  publisher = {{IOP} Publishing},
}

@Article{Yan2021,
  author    = {Yexin Yan and Terrence Stewart and Xuan Choo and Bernhard Vogginger and Johannes Partzsch and Sebastian Hoeppner and Florian Kelber and Chris Eliasmith and Steve Furber and Christian Mayr},
  journal   = {Neuromorphic Computing and Engineering},
  title     = {Comparing Loihi with a {SpiNNaker} 2 Prototype on Low-Latency Keyword Spotting and Adaptive Robotic Control},
  year      = {2021},
  month     = {mar},
  doi       = {10.1088/2634-4386/abf150},
  publisher = {{IOP} Publishing},
}

@Article{Bennett1982,
  author     = {Charles H. Bennett},
  journal    = {International Journal of Theoretical Physics},
  title      = {The thermodynamics of computation{\textemdash}a review},
  year       = {1982},
  month      = {dec},
  number     = {12},
  pages      = {905--940},
  volume     = {21},
  doi        = {10.1007/bf02084158},
  file       = {:Bennett1982 - The Thermodynamics of Computation_a Review.pdf:PDF},
  keywords   = {thermo, comp, theory, found, review},
  priority   = {prio1},
  publisher  = {Springer Science and Business Media {LLC}},
  readstatus = {next},
}

@Article{Wolpert2019,
  author    = {David H Wolpert},
  journal   = {Journal of Physics A: Mathematical and Theoretical},
  title     = {The stochastic thermodynamics of computation},
  year      = {2019},
  month     = {apr},
  number    = {19},
  pages     = {193001},
  volume    = {52},
  doi       = {10.1088/1751-8121/ab0850},
  file      = {:Wolpert2019 - The Stochastic Thermodynamics of Computation.pdf:PDF},
  keywords  = {thermo, comp, theory, found, review},
  publisher = {{IOP} Publishing},
}

@Article{Bahri2020,
  author    = {Yasaman Bahri and Jonathan Kadmon and Jeffrey Pennington and Sam S. Schoenholz and Jascha Sohl-Dickstein and Surya Ganguli},
  journal   = {Annual Review of Condensed Matter Physics},
  title     = {Statistical Mechanics of Deep Learning},
  year      = {2020},
  month     = {mar},
  number    = {1},
  pages     = {501--528},
  volume    = {11},
  doi       = {10.1146/annurev-conmatphys-031119-050745},
  file      = {:Bahri2020 - Statistical Mechanics of Deep Learning.pdf:PDF},
  keywords  = {thermo, statph, stats, dl, ann, review, theory},
  publisher = {Annual Reviews},
}

@InCollection{Feynman1964,
  author     = {Feynman, R. and Leighton, R. and Sands, M.},
  booktitle  = {The Feynman Lectures on Physics},
  title      = {Chapter 44: The Laws of Thermodynamics},
  year       = {1964},
  chapter    = {44},
  volume     = {1},
  file       = {:Feynman1964 - Chapter 44_ the Laws of Thermodynamics.pdf:PDF},
  keywords   = {physics, thermo, MINDS},
  priority   = {prio1},
  readstatus = {next},
  url        = {https://www.feynmanlectures.caltech.edu/I_toc.html},
}

@Article{Urena2020,
  author    = {Esteban Bermudez Ure{\~{n}}a and Yin Chang and Helen Clark and Bianca Datta and {\'{A}}lvaro Escobar and Mike Hardy and Hendrik Hölscher and Amanda Holt and Golnaz Isapour and Mathias Kolle and Christian Kuttner and Victoria Lloyd and Amina Matt and Anthony McDougal and S{\'{e}}bastien R. Mouchet and Laura Ospina and Andrew Parnell and Thomas G. Parton and Primoz Pirih and Alex Qiu and Lukas Schertel and Gea Theodora van de Kerkhof and Silvia Vignolini and William Wardley and Diederik Wiersma},
  journal   = {Faraday Discussions},
  title     = {The role of structure: order vs. disorder in bio-photonic systems: general discussion},
  year      = {2020},
  pages     = {233--246},
  volume    = {223},
  doi       = {10.1039/d0fd90015j},
  publisher = {Royal Society of Chemistry ({RSC})},
}

@Article{Dinc2020,
  author    = {Niyazi Ulas Dinc and Demetri Psaltis and Daniel Brunner},
  journal   = {Photoniques},
  title     = {Optical neural networks: The 3D connection},
  year      = {2020},
  month     = {sep},
  number    = {104},
  pages     = {34--38},
  doi       = {10.1051/photon/202010434},
  publisher = {{EDP} Sciences},
}

@Article{Chen2019,
  author    = {Xiaowen Chen and Francesco Randi and Andrew M. Leifer and William Bialek},
  journal   = {Physical Review E},
  title     = {Searching for collective behavior in a small brain},
  year      = {2019},
  month     = {may},
  number    = {5},
  pages     = {052418},
  volume    = {99},
  doi       = {10.1103/physreve.99.052418},
  publisher = {American Physical Society ({APS})},
}

@Book{Aaronson2013,
  author    = {Scott Aaronson},
  publisher = {Cambridge University Press},
  title     = {Quantum Computing since Democritus},
  year      = {2013},
  doi       = {10.1017/cbo9780511979309},
  keywords  = {quantum, phil, comp, complexity, theory},
}

@Article{Kulkarni2021,
  author    = {Shruti R. Kulkarni and Maryam Parsa and J. Parker Mitchell and Catherine D. Schuman},
  journal   = {Neurocomputing},
  title     = {Benchmarking the Performance of Neuromorphic and Spiking Neural Network Simulators},
  year      = {2021},
  month     = {mar},
  doi       = {10.1016/j.neucom.2021.03.028},
  publisher = {Elsevier {BV}},
}

@Article{Davies2019,
  author    = {Mike Davies},
  journal   = {Nature Machine Intelligence},
  title     = {Benchmarks for progress in neuromorphic computing},
  year      = {2019},
  month     = {sep},
  number    = {9},
  pages     = {386--388},
  volume    = {1},
  doi       = {10.1038/s42256-019-0097-1},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Besold2017,
  author        = {Tarek R. Besold and Artur d'Avila Garcez and Sebastian Bader and Howard Bowman and Pedro Domingos and Pascal Hitzler and Kai-Uwe Kuehnberger and Luis C. Lamb and Daniel Lowd and Priscila Machado Vieira Lima and Leo de Penning and Gadi Pinkas and Hoifung Poon and Gerson Zaverucha},
  title         = {Neural-Symbolic Learning and Reasoning: A Survey and Interpretation},
  year          = {2017},
  month         = nov,
  abstract      = {The study and understanding of human behaviour is relevant to computer science, artificial intelligence, neural computation, cognitive science, philosophy, psychology, and several other areas. Presupposing cognition as basis of behaviour, among the most prominent tools in the modelling of behaviour are computational-logic systems, connectionist models of cognition, and models of uncertainty. Recent studies in cognitive science, artificial intelligence, and psychology have produced a number of cognitive models of reasoning, learning, and language that are underpinned by computation. In addition, efforts in computer science research have led to the development of cognitive computational systems integrating machine learning and automated reasoning. Such systems have shown promise in a range of applications, including computational biology, fault diagnosis, training and assessment in simulators, and software verification. This joint survey reviews the personal ideas and views of several researchers on neural-symbolic learning and reasoning. The article is organised in three parts: Firstly, we frame the scope and goals of neural-symbolic computation and have a look at the theoretical foundations. We then proceed to describe the realisations of neural-symbolic computation, systems, and applications. Finally we present the challenges facing the area and avenues for further research.},
  archiveprefix = {arXiv},
  eprint        = {1711.03902},
  file          = {:http\://arxiv.org/pdf/1711.03902v1:PDF},
  primaryclass  = {cs.AI},
}

@InProceedings{Papadimitriou2019,
  author     = {Christos H. Papadimitriou and Santosh S. Vempala and Daniel Mitropolsky and Michael J. Collins and Wolfgang Maass and Larry F. Abbott},
  booktitle  = {2019 Conference on Cognitive Computational Neuroscience},
  title      = {A Calculus for Brain Computation},
  year       = {2019},
  publisher  = {Cognitive Computational Neuroscience},
  doi        = {10.32470/ccn.2019.1381-0},
  file       = {:Papadimitriou2019 - A Calculus for Brain Computation.pdf:PDF},
  keywords   = {comp, theory, neuromorphic, neuroscience, cogn},
  readstatus = {read},
}

@Article{Findling2021,
  author     = {Charles Findling and Valentin Wyart},
  journal    = {Current Opinion in Behavioral Sciences},
  title      = {Computation noise in human learning and decision-making: origin, impact, function},
  year       = {2021},
  month      = {apr},
  pages      = {124--132},
  volume     = {38},
  doi        = {10.1016/j.cobeha.2021.02.018},
  file       = {:Findling2021 - Computation Noise in Human Learning and Decision Making_ Origin, Impact, Function.pdf:PDF},
  keywords   = {cogn, noise},
  publisher  = {Elsevier {BV}},
  readstatus = {next},
}

@Article{Papadimitriou2020,
  author     = {Papadimitriou, Christos H. and Vempala, Santosh S. and Mitropolsky, Daniel and Collins, Michael and Maass, Wolfgang},
  journal    = {Proceedings of the National Academy of Sciences},
  title      = {Brain computation by assemblies of neurons},
  year       = {2020},
  issn       = {0027-8424},
  number     = {25},
  pages      = {14464--14472},
  volume     = {117},
  abstract   = {Our expanding understanding of the brain at the level of neurons and synapses, and the level of cognitive phenomena such as language, leaves a formidable gap between these two scales. Here we introduce a computational system which promises to bridge this gap: the Assembly Calculus. It encompasses operations on assemblies of neurons, such as project, associate, and merge, which appear to be implicated in cognitive phenomena, and can be shown, analytically as well as through simulations, to be plausibly realizable at the level of neurons and synapses. We demonstrate the reach of this system by proposing a brain architecture for syntactic processing in the production of language, compatible with recent experimental results.Assemblies are large populations of neurons believed to imprint memories, concepts, words, and other cognitive information. We identify a repertoire of operations on assemblies. These operations correspond to properties of assemblies observed in experiments, and can be shown, analytically and through simulations, to be realizable by generic, randomly connected populations of neurons with Hebbian plasticity and inhibition. Assemblies and their operations constitute a computational model of the brain which we call the Assembly Calculus, occupying a level of detail intermediate between the level of spiking neurons and synapses and that of the whole brain. The resulting computational system can be shown, under assumptions, to be, in principle, capable of carrying out arbitrary computations. We hypothesize that something like it may underlie higher human cognitive functions such as reasoning, planning, and language. In particular, we propose a plausible brain architecture based on assemblies for implementing the syntactic processing of language in cortex, which is consistent with recent experimental results.},
  doi        = {10.1073/pnas.2001893117},
  eprint     = {https://www.pnas.org/content/117/25/14464.full.pdf},
  file       = {:Papadimitriou14464 - Brain Computation by Assemblies of Neurons.pdf:PDF},
  keywords   = {theory, comp, neuromorphic, cogn, unconv},
  priority   = {prio1},
  publisher  = {National Academy of Sciences},
  readstatus = {reading},
  url        = {https://www.pnas.org/content/117/25/14464},
}

@Article{Papadimitriou2018,
  author    = {Papadimitriou, Christos H. and Vempala, Santosh S.},
  title     = {Random Projection in the Brain and Computation with Assemblies of Neurons},
  year      = {2018},
  copyright = {Creative Commons Attribution 3.0 Unported license (CC-BY 3.0)},
  doi       = {10.4230/LIPICS.ITCS.2019.57},
  file      = {:Papadimitriou2018 - Random Projection in the Brain and Computation with Assemblies of Neurons.pdf:PDF},
  keywords  = {theory, comp, neuromorphic, cogn, unconv},
  language  = {en},
  publisher = {Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik GmbH, Wadern/Saarbruecken, Germany},
}

@Misc{Gomes2017,
  author = {Lee Gomes},
  title  = {Neuromorphic Chips Are Destined for Deep Learning—or Obscurity},
  year   = {2017},
  url    = {https://spectrum.ieee.org/semiconductors/design/neuromorphic-chips-are-destined-for-deep-learningor-obscurity},
}

@Article{Olah2017,
  author     = {Olah, Chris and Carter, Shan},
  journal    = {Distill},
  title      = {Research Debt},
  year       = {2017},
  note       = {https://distill.pub/2017/research-debt},
  doi        = {10.23915/distill.00005},
  priority   = {prio1},
  readstatus = {read},
}

@Article{Mordvintsev2020,
  author  = {Mordvintsev, Alexander and Randazzo, Ettore and Niklasson, Eyvind and Levin, Michael},
  journal = {Distill},
  title   = {Growing Neural Cellular Automata},
  year    = {2020},
  note    = {https://distill.pub/2020/growing-ca},
  doi     = {10.23915/distill.00023},
}

@Article{Niklasson2021,
  author  = {Niklasson, Eyvind and Mordvintsev, Alexander and Randazzo, Ettore and Levin, Michael},
  journal = {Distill},
  title   = {Self-Organising Textures},
  year    = {2021},
  note    = {https://distill.pub/selforg/2021/textures},
  doi     = {10.23915/distill.00027.003},
}

@Article{Randazzo2021,
  author  = {Randazzo, Ettore and Mordvintsev, Alexander and Niklasson, Eyvind and Levin, Michael},
  journal = {Distill},
  title   = {Adversarial Reprogramming of Neural Cellular Automata},
  year    = {2021},
  note    = {https://distill.pub/selforg/2021/adversarial},
  doi     = {10.23915/distill.00027.004},
}

@Article{Goh2021,
  author  = {Goh, Gabriel and †, Nick Cammarata and †, Chelsea Voss and Carter, Shan and Petrov, Michael and Schubert, Ludwig and Radford, Alec and Olah, Chris},
  journal = {Distill},
  title   = {Multimodal Neurons in Artificial Neural Networks},
  year    = {2021},
  note    = {https://distill.pub/2021/multimodal-neurons},
  doi     = {10.23915/distill.00030},
}

@Article{JohnsonLaird2010,
  author     = {P. N. Johnson-Laird},
  journal    = {Proceedings of the National Academy of Sciences},
  title      = {Mental models and human reasoning},
  year       = {2010},
  month      = {oct},
  number     = {43},
  pages      = {18243--18250},
  volume     = {107},
  doi        = {10.1073/pnas.1012933107},
  file       = {:JohnsonLaird2010 - Mental Models and Human Reasoning.pdf:PDF},
  keywords   = {cogn, logic},
  publisher  = {Proceedings of the National Academy of Sciences},
  readstatus = {read},
}

@Book{Dyson2020,
  author    = {Dyson, George},
  publisher = {Penguin Books},
  title     = {Analogia: the entangled destinies of nature, human beings and machines},
  year      = {2020},
  address   = {London},
  isbn      = {0141975466},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 AutomaticKeywordGroup:Priority\;0\;priority\;,\;>\;1\;0x8a8a8aff\;\;\;;
1 AutomaticKeywordGroup:Status\;2\;readstatus\;,\;>\;1\;0x999999ff\;fast-forward\;\;;
2 KeywordGroup:not read\;0\;readstatus\;^$\;0\;1\;1\;0x8a8a8aff\;\;\;;
1 AutomaticKeywordGroup:Keywords\;0\;keywords\;,\;>\;1\;0x8a8a8aff\;\;\;;
}
